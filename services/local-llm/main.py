import httpx
from common.base_service import BaseService
from common.models import PayloadType, TaskMessage, Data
from fastapi import HTTPException
import os
import requests


class LocalModelService(BaseService):
    """Local Model Service —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Ollama"""
    
    def __init__(self):
        super().__init__("local-model-service", "1.0")
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        self.ollama_host = os.getenv("OLLAMA_HOST", "http://ollama:11434")
        self.model_name = os.getenv("LOCAL_MODEL", "llama2")  # –∏–ª–∏ "mistral", "codellama" –∏ —Ç.–¥.
        self.timeout = int(os.getenv("MODEL_TIMEOUT", str(60*5)))

        # –°–æ–∑–¥–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç httpx
        self.client = httpx.AsyncClient(timeout=self.timeout)
    
    def _can_handle_task_type(self, task_type: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –º–æ–∂–µ—Ç –ª–∏ —Å–µ—Ä–≤–∏—Å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–∏–ø –∑–∞–¥–∞—á–∏"""
        supported_task_types = [
            "generate_response", 
            "text_generation",
            "code_generation"
        ]
        return task_type in supported_task_types

    def _health_handler(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞ –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        try:
            # –õ–æ–≥–∏—Ä—É–µ–º –ø–æ–ø—ã—Ç–∫—É –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
            health_check_url = f"{self.ollama_host}/api/tags"
            print(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è Ollama...")
            print(f"   –•–æ—Å—Ç: {self.ollama_host}")
            print(f"   –ú–æ–¥–µ–ª—å: {self.model_name}")
            print(f"   URL –∑–∞–ø—Ä–æ—Å–∞: {health_check_url}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama
            response = requests.get(health_check_url, timeout=5)
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ HTTP-–æ—Ç–≤–µ—Ç–µ
            print(f"‚úÖ HTTP –∑–∞–ø—Ä–æ—Å –≤—ã–ø–æ–ª–Ω–µ–Ω")
            print(f"   –°—Ç–∞—Ç—É—Å –∫–æ–¥: {response.status_code}")
            print(f"   –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {response.elapsed.total_seconds():.2f} —Å–µ–∫—É–Ω–¥")
            
            models_available = response.status_code == 200
            
            if models_available:
                print(f"üéâ Ollama –¥–æ—Å—Ç—É–ø–µ–Ω –∏ –æ—Ç–≤–µ—á–∞–µ—Ç!")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –Ω–∞—à–∞ –º–æ–¥–µ–ª—å (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
                try:
                    models_data = response.json()
                    all_models = [model['name'] for model in models_data.get('models', [])]
                    model_loaded = any(self.model_name in model_name for model_name in all_models)
                    
                    print(f"üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ Ollama: {', '.join(all_models) if all_models else '–ù–µ—Ç –º–æ–¥–µ–ª–µ–π'}")
                    print(f"üîé –ò—â–µ–º –º–æ–¥–µ–ª—å '{self.model_name}': {'–ù–ê–ô–î–ï–ù–ê' if model_loaded else '–ù–ï –ù–ê–ô–î–ï–ù–ê'}")
                    
                    if not model_loaded:
                        print(f"‚ö†Ô∏è  –í–Ω–∏–º–∞–Ω–∏–µ: –ú–æ–¥–µ–ª—å '{self.model_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ Ollama!")
                        print(f"   –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É: ollama pull {self.model_name}")
                except Exception as parse_error:
                    print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç Ollama: {str(parse_error)}")
                    print(f"   –û—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞: {response.text[:200]}...")
                    model_loaded = False
            else:
                print(f"‚ùå Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω! –°—Ç–∞—Ç—É—Å –∫–æ–¥: {response.status_code}")
                print(f"   –û—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞: {response.text[:200]}...")
                model_loaded = False
            
            status = "ok" if (models_available and model_loaded) else "unhealthy"
            
            result = {
                "status": status,
                "service": self.service_name,
                "model": self.model_name,
                "ollama_available": models_available,
                "model_loaded": model_loaded,
                "host": self.ollama_host,
                "http_status": response.status_code,
                "response_time_seconds": response.elapsed.total_seconds(),
                "available_models": all_models if models_available else []
            }
            
            print(f"üìä –ò—Ç–æ–≥–æ–≤—ã–π —Å—Ç–∞—Ç—É—Å: {status.upper()}")
            print("-" * 50)
            
            return result
            
        except requests.exceptions.Timeout:
            error_msg = f"–¢–∞–π–º–∞—É—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama ({self.ollama_host}) —á–µ—Ä–µ–∑ 5 —Å–µ–∫—É–Ω–¥"
            print(f"‚è∞ {error_msg}")
            print(f"   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ:")
            print(f"   1. –ó–∞–ø—É—â–µ–Ω –ª–∏ Ollama –Ω–∞ —Ö–æ—Å—Ç–µ: ollama serve")
            print(f"   2. –ü—Ä–∞–≤–∏–ª—å–Ω–æ –ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω OLLAMA_HOST: —Å–µ–π—á–∞—Å '{self.ollama_host}'")
            print(f"   3. –î–æ—Å—Ç—É–ø–µ–Ω –ª–∏ –ø–æ—Ä—Ç 11434 –∏–∑ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞")
            
            return {
                "status": "error",
                "service": self.service_name,
                "error": error_msg,
                "error_type": "Timeout",
                "host": self.ollama_host,
                "model": self.model_name
            }
            
        except requests.exceptions.ConnectionError as e:
            error_msg = f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama ({self.ollama_host})"
            print(f"üîå {error_msg}")
            print(f"   –î–µ—Ç–∞–ª–∏: {str(e)}")
            print(f"   –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
            print(f"   1. Ollama –Ω–µ –∑–∞–ø—É—â–µ–Ω–∞. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: ollama serve")
            print(f"   2. –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∞–¥—Ä–µ—Å. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ OLLAMA_HOST: —Å–µ–π—á–∞—Å '{self.ollama_host}'")
            print(f"   3. –û–≥–Ω–µ–Ω–Ω–∞—è —Å—Ç–µ–Ω–∞ –±–ª–æ–∫–∏—Ä—É–µ—Ç –ø–æ—Ä—Ç 11434")
            
            return {
                "status": "error",
                "service": self.service_name,
                "error": error_msg,
                "error_type": "ConnectionError",
                "details": str(e),
                "host": self.ollama_host,
                "model": self.model_name
            }
            
        except requests.exceptions.RequestException as e:
            error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Ollama: {str(e)}"
            print(f"üö® {error_msg}")
            
            return {
                "status": "error",
                "service": self.service_name,
                "error": error_msg,
                "error_type": type(e).__name__,
                "host": self.ollama_host,
                "model": self.model_name
            }
            
        except Exception as e:
            error_msg = f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}"
            print(f"üí• {error_msg}")
            import traceback
            traceback.print_exc()
            
            return {
                "status": "error",
                "service": self.service_name,
                "error": error_msg,
                "error_type": type(e).__name__,
                "host": self.ollama_host,
                "model": self.model_name
            }

    async def _validate_task(self, task_message: TaskMessage):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–¥–∞—á–∏ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞"""
        print(f" Type message: {task_message.data.payload_type}")
        
        if task_message.data.payload_type != PayloadType.TEXT:
            raise HTTPException(
                status_code=400, 
                detail=f"Unsupported payload type: {task_message.data.payload_type}"
            )
        
        prompt = task_message.data.payload.get("text", "")
        if not prompt:
            raise HTTPException(status_code=400, detail="Prompt is required")

    async def _process_task_logic(self, task_message: TaskMessage) -> Data:
        """–õ–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á–∏ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å"""
        prompt = task_message.data.payload.get("text", "")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        max_tokens = task_message.data.payload.get("max_tokens", 512)
        temperature = task_message.data.payload.get("temperature", 0.7)
        
        # –í—ã–∑—ã–≤–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
        response = await self._call_local_model(prompt, max_tokens, temperature)
        
        return Data(
            payload_type=PayloadType.TEXT,
            payload={
                "task": "response_generation",
                "original_prompt": prompt,
                "text": response,
                "model_used": self.model_name,
                "parameters": {
                    "max_tokens": max_tokens,
                    "temperature": temperature
                }
            },
            execution_metadata={
                "task_type": "generate_response",
                "service": "local-model-service",
                "model": self.model_name
            }
        )
    
    async def _call_local_model(self, prompt: str, max_tokens: int, temperature: float) -> str:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ httpx"""
        try:
            url = f"{self.ollama_host}/api/generate"
            
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "num_predict": max_tokens,
                    "temperature": temperature
                }
            }
            
            response = await self.client.post(url, json=payload)
            response.raise_for_status()
            result = response.json()
            return result.get("response", "").strip()
                
        except httpx.HTTPStatusError as e:
            raise HTTPException(
                status_code=503, 
                detail=f"Ollama API error: {str(e)}"
            )
        except httpx.RequestError as e:
            raise HTTPException(
                status_code=503, 
                detail=f"Ollama service unavailable: {str(e)}"
            )

    async def __del__(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ–º –∫–ª–∏–µ–Ω—Ç –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏"""
        await self.client.aclose()

# –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–∏—Å
service = LocalModelService()

if __name__ == "__main__":
    service.run()