from common.base_service import BaseService
from common.models import PayloadType, TaskMessage, Data
from fastapi import HTTPException
import asyncio
from typing import Dict, Any

# –¢–ï–°–¢–û–í–ê–Ø –ó–ê–î–ï–†–ñ–ö–ê –£–ö–ê–ó–´–í–ê–¢–¨
TESTING_SLEEP = 2

class LLMService(BaseService):
    """LLM Service —Å —Ç–µ—Å—Ç–æ–≤–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π"""
    
    def __init__(self):
        super().__init__("llm-service", "0.3")
    
    def _can_handle_task_type(self, task_type: str) -> bool:
        """
        –û–ü–†–ï–î–ï–õ–Ø–ï–¢, –ú–û–ñ–ï–¢ –õ–ò LLM –°–ï–†–í–ò–° –û–ë–†–ê–ë–û–¢–ê–¢–¨ –¢–ò–ü –ó–ê–î–ê–ß–ò
        
        –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ù–£–ñ–ù–û –î–û–ë–ê–í–ò–¢–¨ - –æ–Ω —Ç–µ–ø–µ—Ä—å –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –≤ BaseService
        """
        supported_task_types = [
            "analyze_text", 
            "prompt_response",
        ]
        return task_type in supported_task_types
    
    async def _validate_task(self, task_message: TaskMessage):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–¥–∞—á–∏ –¥–ª—è LLM —Å–µ—Ä–≤–∏—Å–∞"""
        # –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ–º _can_handle_task_type –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        # if not self._can_handle_task_type(task_message.data.task_type):
        #     raise HTTPException(
        #         status_code=400,
        #         detail=f"LLM service does not support task type: {task_message.data.task_type}"
        #     )
        pass
    
    async def _process_task_logic(self, task_message: TaskMessage) -> Data:
        """–õ–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á–∏ –¥–ª—è LLM —Å–µ—Ä–≤–∏—Å–∞"""
        await asyncio.sleep(TESTING_SLEEP)  # –ò–º–∏—Ç–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        
        task_type = task_message.data.task_type
        input_data = task_message.data
        
        if task_type == "analyze_text":
            result = await self._analyze_text(input_data)
        elif task_type == "generate_response":
            result = await self._generate_response(input_data)
        else:
            result = await self._analyze_text(input_data)
        
        print(f" üôè {result}")
        
        return Data(
            payload_type = PayloadType.TEXT,
            payload = result,
            execution_metadata={
                "processing_time_ms": 150.0,
                "task_type": task_type,
                "service": "llm-service"
            }
        )
    
    async def _analyze_text(self, input_data: Data) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        text = input_data.payload.get('text', '')
        language = input_data.payload.get("language", "ru")
        
        words = text.split()
        
        prompt = f"{text} \nin this is text word:{len(words)}"

        return {  #TODO –ù–ï –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç—Å—è –¥–∞–Ω–Ω—ã–µ –ø–æ—á–µ–º—É —Ç–æ
            "task": "text_analysis",
            "word_count": len(words),
            "text": prompt,
            "language": language,
            "estimated_reading_time_sec": max(1, len(words) // 3),
            "contains_questions": "?" in text,
            "sample_analysis": {
                "sentiment": "positive" if any(word in text.lower() for word in ["—Ö–æ—Ä–æ—à", "–æ—Ç–ª–∏—á", "–ø—Ä–µ–∫—Ä–∞—Å"]) else "neutral"
            }
        }
    
    async def _generate_response(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞"""
        prompt = input_data.get("prompt", "")
        
        return {
            "task": "response_generation",
            "original_prompt": prompt,
            "text": f"–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –Ω–∞: '{prompt}'. [–ó–¥–µ—Å—å –±—É–¥–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–π LLM]",
            "response_length": len(prompt) + 50
        }


# –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–∏—Å
service = LLMService()

if __name__ == "__main__":
    service.run()