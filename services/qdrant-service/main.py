# qdrant_service.py
import functools
import os
import uuid
import tempfile
import aiofiles
import httpx
import asyncio
import hashlib
from fastapi import HTTPException
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple

from common.base_service import BaseService
from common.models import PayloadType, TaskMessage, Data

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
try:
    from sentence_transformers import SentenceTransformer
except Exception:
    SentenceTransformer = None

try:
    from qdrant_client import QdrantClient
    from qdrant_client.http import models as qmodels
except Exception:
    QdrantClient = None
    qmodels = None

# –î–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è PDF (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
try:
    import fitz  # PyMuPDF
except Exception:
    fitz = None

# -------------------------
# –ö–æ–Ω—Ñ–∏–≥ –ø–æ –æ–∫—Ä—É–∂–µ–Ω–∏—é
# -------------------------
EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL", "sentence-transformers/paraphrase-multilingual-mpnet-base-v2")
QDRANT_HOST = os.getenv("QDRANT_HOST", "qdrant")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY", "")
QDRANT_COLLECTION = os.getenv("QDRANT_COLLECTION", "documents")
EMBEDDING_BATCH_SIZE = int(os.getenv("EMBEDDING_BATCH_SIZE", "32"))
CHUNK_SIZE_CHARS = int(os.getenv("CHUNK_SIZE_CHARS", "3000"))
CHUNK_OVERLAP_CHARS = int(os.getenv("CHUNK_OVERLAP_CHARS", "500"))
MAX_DOWNLOAD_SIZE = int(os.getenv("MAX_DOWNLOAD_SIZE_BYTES", str(200 * 1024 * 1024)))  # 200MB
DOWNLOAD_TIMEOUT = int(os.getenv("DOWNLOAD_TIMEOUT", "60"))

# -------------------------
# –°–µ—Ä–≤–∏—Å
# -------------------------
class QdrantService(BaseService):
    """
    –°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Qdrant: —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤/–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ -> —á–∞–Ω–∫–∏–Ω–≥ -> —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ -> upsert –≤ Qdrant.
    –¢–∞–∫–∂–µ —É–º–µ–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É (embedding-based search) –∏ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏.
    """

    def __init__(self):
        super().__init__("qdrant-service", "1.0")

        # httpx –∫–ª–∏–µ–Ω—Ç (async) –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏ –≤—ã–∑–æ–≤–∞ –≤–Ω–µ—à–Ω–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
        self.client = httpx.AsyncClient(timeout=DOWNLOAD_TIMEOUT)

        # –•—Ä–∞–Ω–µ–Ω–∏—è –∫–µ—à–∞ –º–æ–¥–µ–ª–∏
        cache_dir = os.getenv("MODEL_CACHE_DIR", "./model_cache")

        # –ó–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (–ª–æ–∫–∞–ª—å–Ω–æ, blocking)
        if SentenceTransformer is None:
            raise RuntimeError("sentence-transformers is required but not installed. Install sentence-transformers.")
        print(f"üîÑ Loading embedding model: {EMBEDDING_MODEL_NAME}")
        # –î–æ–±–∞–≤–ª—è–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç cache_folder
        self.embed_model = SentenceTransformer(
            EMBEDDING_MODEL_NAME, 
            cache_folder=cache_dir
        )
        print("‚úÖ Embedding model loaded")
        print(f"‚úÖ Embedding model loaded: {EMBEDDING_MODEL_NAME}")

        # Qdrant client (sync) - –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ executor –¥–ª—è async –≤—ã–∑–æ–≤–æ–≤
        if QdrantClient is None:
            raise RuntimeError("qdrant-client is required but not installed. Install qdrant-client.")
        print(f"üîÑ Connecting to Qdrant at {QDRANT_HOST}:{QDRANT_PORT}")
        if QDRANT_API_KEY:
            self.qdrant = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT, api_key=QDRANT_API_KEY)
        else:
            self.qdrant = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
        # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        try:
            self._ensure_collection_sync()
            print(f"‚úÖ Qdrant collection '{QDRANT_COLLECTION}' ready")
        except Exception as e:
            print(f"‚ùå Failed to ensure Qdrant collection: {e}")
            raise

        # –õ–∏–º–∏—Ç—ã/–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.chunk_size = CHUNK_SIZE_CHARS
        self.chunk_overlap = CHUNK_OVERLAP_CHARS
        self.embedding_batch = EMBEDDING_BATCH_SIZE

        # –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
        self.ALLOWED_EXTENSIONS = {".pdf", ".txt",} #".md", ".docx", ".rtf", ".html", ".htm"}
        self.MIME_TO_EXTENSION = {
            "application/pdf": ".pdf",
            "text/plain": ".txt",
            "text/markdown": ".md",
            "application/msword": ".doc",
            "application/vnd.openxmlformats-officedocument.wordprocessingml.document": ".docx",
            "text/rtf": ".rtf",
            "text/html": ".html",
            "application/octet-stream": ".bin"
        }


    def _can_handle_task_type(self, task_type: str) -> bool:
        supported = [
            "index_document",
            "index_text",
            "search",
            "reindex_document",
        ]
        return task_type in supported

    def _health_handler(self):
        try:
            return {
                "status": "ok",
                "service": self.service_name,
                "qdrant": {"host": QDRANT_HOST, "port": QDRANT_PORT, "collection": QDRANT_COLLECTION},
                "embedding_model": EMBEDDING_MODEL_NAME
            }
        except Exception as e:
            return {"status": "error", "error": str(e)}

    async def _validate_task(self, task_message: TaskMessage):
        if task_message.data.payload_type == PayloadType.TEXT:
            text = task_message.data.payload.get("text", "")
            if not text or not text.strip():
                raise HTTPException(status_code=400, detail="Text is required for indexing")
            
        elif task_message.data.payload_type == PayloadType.FILE:
            # –æ–∂–∏–¥–∞–µ–º file_url
            file_url = task_message.data.payload.get("file_url", "")
            if not file_url:
                raise HTTPException(status_code=400, detail="file_url is required for index_document")
            
        else:
            raise HTTPException(status_code=400, detail="Unsupported task_type")

    # -------------------------
    # –õ–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á–∏ 
    # ------------------------- 
    async def _process_task_logic(self, task_message: TaskMessage) -> Data:
        task_type = task_message.data.task_type
        payload_type = task_message.data.payload_type
        if payload_type == PayloadType.FILE:
            return await self._handle_index_document(task_message)
        elif payload_type == PayloadType.TEXT and task_type == "index_text":
            return await self._handle_index_text(task_message)
        elif payload_type == PayloadType.TEXT:
            return await self._handle_search(task_message)
        else:
            raise HTTPException(status_code=400, detail=f"Unknown task_type: {task_type} or/and no support type payload{payload_type}")

    # ---------------------------------------------------------
    # –û–±—Ä–∞–±–æ—Ç–∫–∞
    # ---------------------------------------------------------
    async def _process_text_content(self, text: str, doc_id: str, owner: str, origin_url: Optional[str] = None) -> Dict[str, Any]:
        """–û–±—â–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞: —á–∞–Ω–∫–∏–Ω–≥ -> —ç–º–±–µ–¥–¥–∏–Ω–≥ -> upsert"""
        if not text or not text.strip():
             raise HTTPException(status_code=400, detail="Empty text content")

        # –ß–∞–Ω–∫–∏–Ω–≥
        chunks = self._chunk_text(text, chunk_size=self.chunk_size, overlap=self.chunk_overlap)
        checksum = hashlib.sha256(text.encode("utf-8")).hexdigest()
        
        points = []
        previews = [] # –î–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Å–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç—ã –æ—Ç–¥–µ–ª—å–Ω–æ
        
        for idx, (chunk_text, start_offset) in enumerate(chunks):
            # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤
            chunk_text_str = str(chunk_text)
            
            qdrant_point_id = str(uuid.uuid4())
            original_chunk_id = f"{doc_id}::chunk::{idx}::{uuid.uuid4().hex}"

            payload_meta = {
                "doc_id": doc_id,
                "owner": owner,
                "offset": start_offset,
                "chunk_index": idx,
                "checksum": checksum,
                "text_preview": chunk_text_str[:500], # Magic number -> constant
                "source_id": original_chunk_id
            }
            if origin_url:
                payload_meta["origin_url"] = origin_url

            points.append({"id": qdrant_point_id, "text": chunk_text_str, "payload": payload_meta})
            previews.append(chunk_text_str)

        # –≠–º–±–µ–¥–¥–∏–Ω–≥ (batch processing)
        embeddings = await self._embed_texts(previews)

        # –°–±–æ—Ä–∫–∞ –¥–ª—è Qdrant
        q_points = []
        for p, emb in zip(points, embeddings):
            vec = emb.tolist() if hasattr(emb, "tolist") else list(emb)
            q_points.append({"id": p["id"], "vector": vec, "payload": p["payload"]})

        # Upsert
        upsert_result = await self._qdrant_upsert(q_points)
        
        return {
            "doc_id": doc_id,
            "chunks_count": len(q_points),
            "upsert_result": upsert_result
        }

    # ---------------------------------------------------------
    # Refactored Handlers
    # ---------------------------------------------------------
    async def _handle_index_document(self, task_message: TaskMessage) -> Data:
        payload = task_message.data.payload
        file_url = payload.get("file_url")
        owner = payload.get("owner", "unknown")
        doc_id = payload.get("doc_id") or f"doc-{uuid.uuid4().hex}"

        temp_path = await self._download_file(file_url)
        try:
            text = await self._extract_text_from_file(temp_path)
        finally:
            if os.path.exists(temp_path):
                os.unlink(temp_path)

        result = await self._process_text_content(text, doc_id, owner, origin_url=file_url)

        return Data(
            payload_type=PayloadType.TEXT,
            task_type="index_document",
            payload={**result, "task": "index_document"},
            execution_metadata={"service": self.service_name}
        )

    async def _handle_index_text(self, task_message: TaskMessage) -> Data:
        payload = task_message.data.payload
        text = payload.get("text", "")
        owner = payload.get("owner", "unknown")
        doc_id = payload.get("doc_id") or f"doc-{uuid.uuid4().hex}"

        result = await self._process_text_content(text, doc_id, owner)

        return Data(
            payload_type=PayloadType.TEXT,
            task_type="index_text",
            payload={**result, "task": "index_text"},
            execution_metadata={"service": self.service_name}
        )


    # -------------------------
    # Search flow
    # -------------------------
    async def _handle_search(self, task_message: TaskMessage) -> Data:
        payload = task_message.data.payload
        query = payload.get("text", "").strip()
        top_k = int(payload.get("top_k", 3))

        # 1. –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞
        if not query:
            raise HTTPException(status_code=400, detail="Search query cannot be empty.")

        # 2. –≠–º–±–µ–¥–¥–∏–Ω–≥
        try:
            q_embs = await self._embed_texts([query])
            q_emb = q_embs[0]
        except Exception as e:
             raise HTTPException(status_code=500, detail=f"Embedding generation failed: {str(e)}")

        if not self._is_valid_embedding(q_emb):
             raise HTTPException(status_code=400, detail="Generated embedding is invalid (zero vector).")

        # 3. –ü–æ–∏—Å–∫ –≤ Qdrant
        search_results = await self._qdrant_search(
            vector=q_emb,
            top=top_k,
            score_threshold=0.3
        )

        # 4. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ (—Ç–µ–∫—Å—Ç + –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)
        formatted_text, sources_meta = self._format_results(search_results, query)
        
        # –õ–æ–≥–∏–∫–∞ –æ—Ç–≤–µ—Ç–∞: –¥–∞–∂–µ –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º Data, –Ω–æ —Å –ø—É—Å—Ç—ã–º —Ç–µ–∫—Å—Ç–æ–º
        # –∏–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ–º, —á—Ç–æ–±—ã –ø–∞–π–ø–ª–∞–π–Ω –Ω–µ –ø–∞–¥–∞–ª —Å –æ—à–∏–±–∫–æ–π, –∞ LLM –∑–Ω–∞–ª–∞, —á—Ç–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–µ—Ç.
        
        final_payload = {
            "text": formatted_text if search_results else "No relevant context found.",
            "query": query, # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            "found_count": len(search_results)
        }

        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ execution_metadata
        exec_meta = {
            "service": self.service_name,
            "model": EMBEDDING_MODEL_NAME,
            "sources": sources_meta # –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–µ—Ç–∞–ª—è–º–∏ (url, doc_id, score)
        }

        return Data(
            payload_type=PayloadType.TEXT,
            task_type="search_result",
            payload=final_payload,
            execution_metadata=exec_meta
        )

    def _format_results(self, results: List[Dict[str, Any]], query: str) -> Tuple[str, List[Dict[str, Any]]]:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è LLM: —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        """
        if not results:
            return (
                f"No relevant documents found for query: '{query}'. "
                "Possible reasons: documents not indexed, query too specific, or low relevance threshold.",
                []
            )

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è LLM
        context_parts = []
        sources_metadata = []

        # –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è —Å—Å—ã–ª–æ–∫
        sources_index = {}
        for i, hit in enumerate(results):
            payload = hit["payload"]
            source_key = f"{payload.get('doc_id')}_{payload.get('chunk_index')}"
            
            if source_key not in sources_index:
                sources_index[source_key] = {
                    "id": len(sources_index) + 1,
                    "url": payload.get("origin_url"),
                    "doc_id": payload.get("doc_id"),
                    "owner": payload.get("owner")
                }
            
            sources_metadata.append({
                "source_id": sources_index[source_key]["id"],
                "score": round(hit["score"], 4),
                "text_preview": payload.get("text_preview", "")[:200],
                "chunk_index": payload.get("chunk_index")
            })

        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context_parts.append(f"Found {len(results)} relevant fragments for query: '{query}'")
        context_parts.append("\nSources:")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        for source in sources_index.values():
            source_ref = f"[{source['id']}]"
            if source["url"]:
                source_ref += f" URL: {source['url']}"
            else:
                source_ref += f" Document ID: {source['doc_id']}"
            source_ref += f" (Owner: {source['owner']})"
            context_parts.append(source_ref)
        
        context_parts.append("\nRelevant content fragments:")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Å —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        for i, hit in enumerate(results):
            payload = hit["payload"]
            source_key = f"{payload.get('doc_id')}_{payload.get('chunk_index')}"
            source_id = sources_index[source_key]["id"]
            
            fragment = (
                f"Fragment #{i+1} (Relevance: {hit['score']:.3f}, Source: [{source_id}]):\n"
                f"{payload.get('text_preview', '').strip()}"
            )
            context_parts.append(fragment)

        final_text = "\n".join(context_parts)
        return final_text, sources_metadata

    def _is_valid_embedding(self, emb) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ —ç–º–±–µ–¥–¥–∏–Ω–≥ –Ω–µ –≤—ã—Ä–æ–∂–¥–µ–Ω–Ω—ã–π (–Ω–µ –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä)"""
        import numpy as np
        if hasattr(emb, "tolist"):
            emb = emb.tolist()
        return np.linalg.norm(emb) > 0.1  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≤–µ–∫—Ç–æ—Ä–∞

    # -------------------------
    # –£—Ç–∏–ª–∏—Ç—ã
    # -------------------------
    async def _download_file(self, url: str) -> str:
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª –≤–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ.
        –û—Å–Ω–æ–≤–Ω–æ–π –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –ø—Ä–æ—Ü–µ—Å—Å–∞ –∑–∞–≥—Ä—É–∑–∫–∏.
        """
        self._validate_url_security(url)
        original_filename = self._extract_filename_from_url(url)
        
        try:
            tmp_path = await self._stream_file_to_temp(url, original_filename)
            self._validate_downloaded_file(tmp_path)
            return tmp_path
        except Exception as e:
            # –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø—Ä–∏ –ª—é–±–æ–π –æ—à–∏–±–∫–µ
            if 'tmp_path' in locals() and os.path.exists(tmp_path):
                os.unlink(tmp_path)
            raise

    def _validate_url_security(self, url: str):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç URL –Ω–∞ –ø–æ–ø—ã—Ç–∫–∏ SSRF –∞—Ç–∞–∫"""
        parsed = httpx.URL(url)
        blocked_hosts = {"127.0.0.1", "localhost", "0.0.0.0"}
        blocked_suffixes = {".local", ".internal", ".localhost"}
        
        host = parsed.host.lower()
        if host in blocked_hosts or any(host.endswith(suffix) for suffix in blocked_suffixes):
            raise HTTPException(status_code=400, detail="Local network downloads are forbidden")

    def _extract_filename_from_url(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ URL"""
        path = httpx.URL(url).path
        return os.path.basename(path) or "downloaded_file"

    async def _stream_file_to_temp(self, url: str, original_filename: str) -> str:
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É.
        """
        async with self.client.stream("GET", url, timeout=DOWNLOAD_TIMEOUT) as response:
            response.raise_for_status()
            
            file_ext = self._determine_file_extension(
                original_filename,
                response.headers.get("content-type", ""),
                response.headers.get("content-disposition", "")
            )
            
            tmp_path = self._create_temp_file_path(file_ext)
            total_size = await self._write_response_to_file(response, tmp_path)
            
            self._log_download_details(original_filename, file_ext, total_size)
            return tmp_path

    def _determine_file_extension(self, filename: str, content_type: str, content_disposition: str) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞:
        1. Content-Disposition header
        2. Content-Type header
        3. –ò–º—è —Ñ–∞–π–ª–∞ –∏–∑ URL
        """
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: Content-Disposition
        if "filename=" in content_disposition:
            disp_filename = self._parse_content_disposition_filename(content_disposition)
            if disp_filename:
                ext = os.path.splitext(disp_filename)[1].lower()
                if ext in self.ALLOWED_EXTENSIONS:
                    return ext
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: Content-Type
        mime_type = content_type.split(";")[0].strip().lower()
        if mime_type in self.MIME_TO_EXTENSION:
            ext = self.MIME_TO_EXTENSION[mime_type]
            if ext in self.ALLOWED_EXTENSIONS:
                return ext
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3: –ò–º—è —Ñ–∞–π–ª–∞ –∏–∑ URL
        ext = os.path.splitext(filename)[1].lower()
        if ext in self.ALLOWED_EXTENSIONS:
            return ext
        
        return ".bin"  # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π fallback

    def _parse_content_disposition_filename(self, header: str) -> str:
        """–ü–∞—Ä—Å–∏—Ç –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ Content-Disposition"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∞—Å—Ç—å –ø–æ—Å–ª–µ 'filename='
            filename_part = header.split("filename=")[-1].strip()
            # –£–¥–∞–ª—è–µ–º –æ–±—Ä–∞–º–ª—è—é—â–∏–µ –∫–∞–≤—ã—á–∫–∏ –∏ –ø—Ä–æ–±–µ–ª—ã
            return filename_part.strip('"\' ')
        except Exception:
            return ""

    def _create_temp_file_path(self, extension: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
        tmp_dir = tempfile.gettempdir()
        unique_name = f"qdrant_{uuid.uuid4().hex}{extension}"
        return os.path.join(tmp_dir, unique_name)

    async def _write_response_to_file(self, response: httpx.Response, file_path: str) -> int:
        """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ—Ç–≤–µ—Ç–∞ –≤ —Ñ–∞–π–ª —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Ä–∞–∑–º–µ—Ä–∞"""
        total_size = 0
        async with aiofiles.open(file_path, "wb") as f:
            async for chunk in response.aiter_bytes():
                total_size += len(chunk)
                if total_size > MAX_DOWNLOAD_SIZE:
                    await f.close()
                    raise HTTPException(
                        status_code=413,
                        detail=f"File exceeds maximum size of {MAX_DOWNLOAD_SIZE // (1024*1024)} MB"
                    )
                await f.write(chunk)
        return total_size

    def _validate_downloaded_file(self, file_path: str):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å —Å–∫–∞—á–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
        if not os.path.exists(file_path):
            raise HTTPException(status_code=500, detail="File creation failed")
        
        file_size = os.path.getsize(file_path)
        if file_size == 0:
            os.unlink(file_path)
            raise HTTPException(status_code=400, detail="Downloaded file is empty")
        
        if file_size > MAX_DOWNLOAD_SIZE:
            os.unlink(file_path)
            raise HTTPException(status_code=413, detail="File size exceeds limit after download")

    def _log_download_details(self, original_name: str, ext: str, size: int):
        """–õ–æ–≥–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∏"""
        print(f"üì• Downloaded: {original_name} -> {ext} ({size} bytes)")

    async def _extract_text_from_file(self, file_path: str) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞.
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: PDF, TXT
        """
        ext = Path(file_path).suffix.lower()
        
        if ext == ".pdf" and fitz is not None:
            loop = asyncio.get_running_loop()
            return await loop.run_in_executor(None, self._sync_extract_pdf_text, file_path)
        
        elif ext in (".txt", ".text"):
            try:
                async with aiofiles.open(file_path, "r", encoding="utf-8") as f:
                    return await f.read()
            except UnicodeDecodeError:
                # –ü–æ–ø—ã—Ç–∫–∞ —á—Ç–µ–Ω–∏—è —Å fallback-–∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
                async with aiofiles.open(file_path, "r", encoding="latin-1") as f:
                    return await f.read()
        
        raise HTTPException(
            status_code=415,
            detail=f"Unsupported file format: {ext}. Supported formats: .pdf, .txt"
        )

    def _sync_extract_pdf_text(self, file_path: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF —Ñ–∞–π–ª–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º PyMuPDF"""
        try:
            with fitz.open(file_path) as doc:
                return "\n".join(page.get_text("text") for page in doc)
        except Exception as e:
            self.logger.error(f"PDF extraction failed for {file_path}: {str(e)}")
            raise HTTPException(
                status_code=422,
                detail=f"Failed to extract text from PDF: {str(e)}"
            )

    def _chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[Tuple[str, int]]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å —É–º–Ω—ã–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –ø–æ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≥—Ä–∞–Ω–∏—Ü–∞–º.
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            chunk_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
            overlap: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (—Ç–µ–∫—Å—Ç_—á–∞–Ω–∫–∞, –Ω–∞—á–∞–ª—å–Ω–∞—è_–ø–æ–∑–∏—Ü–∏—è_–≤_–∏—Å—Ö–æ–¥–Ω–æ–º_—Ç–µ–∫—Å—Ç–µ)
        """
        if not text.strip():
            return []
        
        chunks = []
        start = 0
        text_length = len(text)
        
        while start < text_length:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω–µ—Ü —Ç–µ–∫—É—â–µ–≥–æ —á–∞–Ω–∫–∞
            end = min(start + chunk_size, text_length)
            
            # –ò—â–µ–º –ª—É—á—à–µ–µ –º–µ—Å—Ç–æ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è, –µ—Å–ª–∏ –Ω–µ –≤ –∫–æ–Ω—Ü–µ —Ç–µ–∫—Å—Ç–∞
            if end < text_length:
                end = self._find_smart_split_point(text, start, end)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —á–∞–Ω–∫, –µ—Å–ª–∏ –æ–Ω –Ω–µ –ø—É—Å—Ç–æ–π
            chunk = text[start:end].strip()
            if chunk:
                chunks.append((chunk, start))
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            if end >= text_length:
                break
            
            # –í—ã—á–∏—Å–ª—è–µ–º –Ω–∞—á–∞–ª–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —á–∞–Ω–∫–∞ —Å —É—á–µ—Ç–æ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
            next_start = end - overlap
            # –ó–∞—â–∏—Ç–∞ –æ—Ç –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏—è –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            if next_start <= start:
                next_start = end
            start = next_start
        
        return chunks

    def _find_smart_split_point(self, text: str, start: int, end: int) -> int:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Ç–æ—á–∫—É —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [start, end].
        –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π: –∞–±–∑–∞—Ü—ã > –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è > –ø—Ä–æ–±–µ–ª—ã.
        """
        # –ó–æ–Ω–∞ –ø–æ–∏—Å–∫–∞: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 20% —Ç–µ–∫—É—â–µ–≥–æ —á–∞–Ω–∫–∞
        search_zone_start = max(start, end - int((end - start) * 0.2))
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –æ—Ç –±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã—Ö –∫ –º–µ–Ω–µ–µ
        separators = ["\n\n", "\n", ". ", "! ", "? ", "; ", " "]
        
        for sep in separators:
            # –ò—â–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å —Å–ø—Ä–∞–≤–∞ –Ω–∞–ª–µ–≤–æ –≤ –∑–æ–Ω–µ –ø–æ–∏—Å–∫–∞
            pos = text.rfind(sep, search_zone_start, end)
            if pos != -1:
                return pos + len(sep)  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–∑–∏—Ü–∏—é –ø–æ—Å–ª–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è
        
        return end  # –ï—Å–ª–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω, —Ä–∞–∑–¥–µ–ª—è–µ–º –∂–µ—Å—Ç–∫–æ


    async def _embed_texts(self, texts: List[str]) -> List[Any]:
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞. –ö–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å.
        """
        norm_texts = self._normalize_inputs(texts)
        loop = asyncio.get_event_loop()
        embeddings = []

        for i in range(0, len(norm_texts), self.embedding_batch):
            batch = norm_texts[i : i + self.embedding_batch]
            batch_embeddings = await self._process_embedding_batch(loop, batch, i)
            embeddings.extend(batch_embeddings)
            
        return embeddings

    def _normalize_inputs(self, texts: List[Any]) -> List[str]:
        """–ü—Ä–∏–≤–æ–¥–∏—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫ —Å—Ç—Ä–æ–∫–∞–º."""
        norm_texts = []
        for i, t in enumerate(texts):
            if isinstance(t, str):
                norm_texts.append(t)
            else:
                try:
                    # –õ—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å logger –≤–º–µ—Å—Ç–æ print
                    print(f"qdrant: coercing input[{i}] type {type(t).__name__} to str")
                    norm_texts.append(str(t))
                except Exception as e:
                    raise RuntimeError(f"Invalid input for embedding at index {i}: {e}")
        return norm_texts

    async def _process_embedding_batch(self, loop, batch: List[str], start_idx: int) -> List[Any]:
        """
        –ü—ã—Ç–∞–µ—Ç—Å—è –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –±–∞—Ç—á —Ü–µ–ª–∏–∫–æ–º. –ü—Ä–∏ –æ—à–∏–±–∫–µ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç –∫ –ø–æ—à—Ç—É—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ.
        """
        # –ß–∞—Å—Ç–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –≤ executor
        func = functools.partial(
            self.embed_model.encode, 
            batch, 
            convert_to_numpy=True, 
            show_progress_bar=False
        )

        try:
            return await loop.run_in_executor(None, func)
        except Exception as e:
            print(f"Batch embedding failed at idx {start_idx}: {e}. Switching to fallback.")
            return await self._process_batch_fallback(loop, batch, start_idx)

    async def _process_batch_fallback(self, loop, batch: List[str], start_idx: int) -> List[Any]:
        """
        –ú–µ–¥–ª–µ–Ω–Ω—ã–π —Ä–µ–∂–∏–º: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ –æ–¥–Ω–æ–º—É, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –±–∏—Ç—ã–π —ç–ª–µ–º–µ–Ω—Ç.
        """
        results = []
        for j, item in enumerate(batch):
            func_single = functools.partial(
                self.embed_model.encode, 
                [item], # encode –æ–∂–∏–¥–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∏–ª–∏ —Å—Ç—Ä–æ–∫—É, –Ω–æ –¥–ª—è consistency –ø–µ—Ä–µ–¥–∞–µ–º —Å–ø–∏—Å–æ–∫
                convert_to_numpy=True, 
                show_progress_bar=False
            )
            try:
                # –†–µ–∑—É–ª—å—Ç–∞—Ç encode –¥–ª—è —Å–ø–∏—Å–∫–∞ ‚Äî —ç—Ç–æ —Å–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤. –ë–µ—Ä–µ–º [0] –∏–ª–∏ extend
                emb = await loop.run_in_executor(None, func_single)
                results.extend(emb)
            except Exception as e:
                global_idx = start_idx + j
                print(f"Single encode failed at index {global_idx}")
                raise RuntimeError(f"Embedding failed for item {global_idx} (len={len(item)})") from e
        return results
    
    def _sync_upsert_safe(self, points_batch: List[Dict]) -> int:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π upsert —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏—Å–∫–ª—é—á–µ–Ω–∏–π"""
        try:
            q_points = self._prepare_points_for_upsert(points_batch)
            self._execute_upsert(q_points)
            return len(q_points)
        except Exception as e:
            self._handle_upsert_error(e, points_batch)
            raise

    def _prepare_points_for_upsert(self, points_batch: List[Dict]) -> List:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Ç–æ—á–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Qdrant"""
        q_points = []
        for point in points_batch:
            q_point = self._convert_single_point(point)
            if q_point:
                q_points.append(q_point)
        return q_points

    def _convert_single_point(self, point: Dict):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –æ–¥–Ω—É —Ç–æ—á–∫—É –≤ —Ñ–æ—Ä–º–∞—Ç Qdrant"""
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞
        vector = point["vector"]
        vector_list = self._convert_vector_to_list(vector)
        if not vector_list:
            return None

        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ—á–∫–∏ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        return qmodels.PointStruct(
            id=str(point["id"]),
            vector=vector_list,
            payload=point.get("payload", {})
        )

    def _convert_vector_to_list(self, vector: Any) -> Optional[List]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤–µ–∫—Ç–æ—Ä –≤ —Å–ø–∏—Å–æ–∫"""
        try:
            if hasattr(vector, "tolist"):
                return vector.tolist()
            elif isinstance(vector, list):
                return [float(x) for x in vector]
            else:
                return None
        except (ValueError, TypeError):
            return None

    def _execute_upsert(self, q_points: List):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç upsert –≤ Qdrant"""
        if not q_points:
            print("‚ö†Ô∏è No valid points to upsert")
            return

        print(f"üì§ Upserting {len(q_points)} points to collection '{QDRANT_COLLECTION}'")
        
        self.qdrant.upsert(
            collection_name=QDRANT_COLLECTION,
            points=q_points,
            wait=True
        )
        
        print(f"‚úÖ Upsert successful: {len(q_points)} points")

    def _handle_upsert_error(self, error: Exception, points_batch: List[Dict]):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—à–∏–±–∫–∏ –ø—Ä–∏ upsert"""
        print(f"‚ùå Upsert failed: {str(error)}")
        import traceback
        traceback.print_exc()
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ç–ª–∞–¥–∫–∞ –æ—à–∏–±–∫–∏
        if self._is_vector_error(error):
            self._debug_vector_issues(points_batch)

    def _is_vector_error(self, error: Exception) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —Å–≤—è–∑–∞–Ω–∞ –ª–∏ –æ—à–∏–±–∫–∞ —Å –≤–µ–∫—Ç–æ—Ä–∞–º–∏"""
        error_msg = str(error).lower()
        return "wrong input data" in error_msg or "vectors" in error_msg

    def _debug_vector_issues(self, points_batch: List[Dict]):
        """–û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –ø—Ä–æ–±–ª–µ–º–∞–º —Å –≤–µ–∫—Ç–æ—Ä–∞–º–∏"""
        print("üîç Vector validation details:")
        for i, point in enumerate(points_batch[:3]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3
            self._print_point_debug_info(point, i)

    def _print_point_debug_info(self, point: Dict, index: int):
        """–í—ã–≤–æ–¥–∏—Ç –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –æ–¥–Ω–æ–π —Ç–æ—á–∫–µ"""
        vector = point.get("vector")
        print(f"  Point #{index}:")
        print(f"    ID: {point.get('id')}")
        print(f"    Vector type: {type(vector)}")
        
        if hasattr(vector, "__len__"):
            print(f"    Vector length: {len(vector)}")
        else:
            print("    Vector length: unknown")
            
        if hasattr(vector, "shape"):
            print(f"    Vector shape: {vector.shape}")

    async def _upsert_points_one_by_one(self, loop, points: List[Dict]) -> int:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥: upsert –ø–æ –æ–¥–Ω–æ–π —Ç–æ—á–∫–µ"""
        success_count = 0
        for point in points:
            try:
                result = await loop.run_in_executor(
                    None,
                    functools.partial(self._sync_upsert_safe, [point])
                )
                success_count += result
            except Exception as e:
                print(f"‚ùå Single point upsert failed for {point.get('id')}: {e}")
        return success_count
 
    async def _qdrant_upsert(self, points: List[Dict[str, Any]], batch_size: int = 100) -> Dict[str, Any]:
        """Upsert —Å –ø–æ–ª–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö"""
        
        validated_points = self._validate_points(points)
        if not validated_points:
            return {"upserted": 0, "error": "No valid points to upsert"}

        success_count = await self._process_batches(validated_points, batch_size)
        return {"upserted": success_count}

    def _validate_points(self, points: List[Dict]) -> List[Dict]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–æ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö"""
        validated_points = []
        for point in points:
            try:
                validated_point = self._validate_single_point(point)
                if validated_point:
                    validated_points.append(validated_point)
            except Exception as e:
                print(f"‚ùå Point validation failed: {e}")
                continue
        return validated_points

    def _validate_single_point(self, point: Dict) -> Optional[Dict]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –æ–¥–Ω–æ–π —Ç–æ—á–∫–∏"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
        if not point.get("id"):
            point["id"] = str(uuid.uuid4())
            
        if not point.get("vector"):
            print(f"‚ö†Ô∏è Skipping point without vector: {point.get('id')}")
            return None
            
        if not isinstance(point["vector"], list) or len(point["vector"]) == 0:
            print(f"‚ö†Ô∏è Skipping point with invalid vector: {point.get('id')}")
            return None
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞
        expected_size = self.embed_model.get_sentence_embedding_dimension()
        if len(point["vector"]) != expected_size:
            print(f"‚ö†Ô∏è Vector size mismatch for point {point.get('id')}: {len(point['vector'])} != {expected_size}")
            return None
            
        return point

    async def _process_batches(self, validated_points: List[Dict], batch_size: int) -> int:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ—á–µ–∫ –±–∞—Ç—á–∞–º–∏"""
        loop = asyncio.get_event_loop()
        success_count = 0

        for i in range(0, len(validated_points), batch_size):
            batch = validated_points[i:i + batch_size]
            batch_success = await self._process_single_batch(loop, batch, i)
            success_count += batch_success
            
        return success_count

    async def _process_single_batch(self, loop, batch: List[Dict], batch_index: int) -> int:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞"""
        try:
            result = await loop.run_in_executor(
                None, 
                functools.partial(self._sync_upsert_safe, batch)
            )
            print(f"‚úÖ Successfully upserted batch {batch_index//len(batch) + 1}: {result} points")
            await asyncio.sleep(0.1)
            return result
        except Exception as e:
            print(f"‚ùå Batch upsert failed at index {batch_index}: {e}")
            # Fallback: upsert –ø–æ –æ–¥–Ω–æ–º—É
            return await self._upsert_points_one_by_one(loop, batch)


    #! –¢–ï–°–¢–û–í–´–ô –ú–ï–¢–û–î –î–õ–Ø –ü–†–û–í–ï–†–ö–ò –í–°–ï–• –î–û–ö–£–ú–ï–ù–¢–û–í –ò–ó –ö–î–†–ê–ù–¢
    def _debug_get_all_documents(self, limit: int = 5):
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ doc_id
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º scroll –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö —Ç–æ—á–µ–∫
            all_points = []
            next_page = None
            while len(all_points) < limit * 10:  # –ë–µ—Ä–µ–º —Å –∑–∞–ø–∞—Å–æ–º
                scroll_result = self.qdrant.scroll(...)
                points = scroll_result.points
                next_page = scroll_result.next_page_offset
                all_points.extend(points)
                if next_page is None:
                    break
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ doc_id
            from collections import defaultdict
            docs = defaultdict(list)
            for point in all_points:
                doc_id = point.payload.get("doc_id")
                if doc_id:
                    docs[doc_id].append(point)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = []
            for doc_id, points in list(docs.items())[:limit]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                if not points:
                    continue
                    
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º —á–∞–Ω–∫–∏ –ø–æ –ø–æ–∑–∏—Ü–∏–∏
                chunks = sorted(points, key=lambda x: x.payload.get('offset', 0))
                last_chunk = chunks[-1]
                last_words = last_chunk.payload.get('text_preview', '').strip().split()[-3:]
                
                result.append({
                    "doc_id": doc_id,
                    "total_chunks": len(points),
                    "first_chunk_preview": chunks[0].payload.get('text_preview', '')[:50] + "...",
                    "last_chunk_preview": last_chunk.payload.get('text_preview', '')[:50] + "...",
                    "last_words": " ".join(last_words),
                    "total_characters": sum(len(p.payload.get('text_preview', '')) for p in points)
                })
            
            return result
            
        except Exception as e:
            print(f"‚ùå DEBUG ERROR: {str(e)}")
            return [{"error": str(e)}]




    async def _qdrant_search(self, vector: Any, top: int = 6, score_threshold: float = 0.3) -> List[Dict[str, Any]]:
        """
        –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –¥–ª—è qdrant-client >= 1.7.0
        """
        loop = asyncio.get_event_loop()

        def _sync_query():
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤–µ–∫—Ç–æ—Ä –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            q_vec = vector.tolist() if hasattr(vector, "tolist") else [float(x) for x in vector]
            
            # === –ü–†–ê–í–ò–õ–¨–ù–´–ô –í–´–ó–û–í –î–õ–Ø –°–û–í–†–ï–ú–ï–ù–ù–û–ì–û QDRANT ===
            resp = self.qdrant.query_points(
                collection_name=QDRANT_COLLECTION,
                query=q_vec,  
                using=None,   
                limit=top,
                with_payload=True,
                score_threshold=score_threshold,  # –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ü–û –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–ò
                with_vectors=False
            )
            
            # === –ü–†–ê–í–ò–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===
            hits = []
            for point in resp.points:
                hits.append({
                    "id": str(point.id),
                    "score": float(point.score),
                    "payload": point.payload or {},
                })
                # –û—Ç–ª–∞–¥–∫–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                preview = point.payload.get('text_preview', '')[:100] if point.payload else ''
                print(f"  üìå Hit (score={point.score:.4f}): {preview}...")
            
            print(f"‚úÖ Found {len(hits)} relevant results (score_threshold={score_threshold})")

            #! –î–ï–ë–ê–ì –ú–û–ú–ï–ù–¢ –°–õ–ï–î–£–ï–¢ –£–ë–†–ê–¢–¨ –≠–¢–û
            print("\n" + "="*60)
            print("üêû DEBUG ALL DOCUMENTS (first 5)")
            print("="*60)
            docs = self._debug_get_all_documents(limit=5)
            
            for i, doc in enumerate(docs):
                print(f"\nüìÑ Document #{i+1}: {doc.get('doc_id', 'N/A')}")
                print(f"   Chunks: {doc.get('total_chunks', 'N/A')}")
                print(f"   First: '{doc.get('first_chunk_preview', '')}'")
                print(f"   Last:  '{doc.get('last_chunk_preview', '')}'")
                print(f"   Final words: '{doc.get('last_words', '')}'")
        
            print("="*60 + "\n")

            return hits

        return await loop.run_in_executor(None, _sync_query)
    
    def _ensure_collection_sync(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        try:
            try:
                self.qdrant.get_collection(collection_name=QDRANT_COLLECTION)
                print(f"‚úÖ Collection '{QDRANT_COLLECTION}' already exists")
                return
            except Exception as e:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ –æ—à–∏–±–∫–∞ "not found", –∏–Ω–∞—á–µ —Ä–µ–π–∑–∏–º
                if "not found" not in str(e).lower() and "404" not in str(e):
                    print(f"‚ö†Ô∏è Collection check warning: {str(e)}")

            vector_size = self.embed_model.get_sentence_embedding_dimension()
            print(f"üîÑ Creating collection '{QDRANT_COLLECTION}' with vector size {vector_size}")

            # –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
            self.qdrant.create_collection(
                collection_name=QDRANT_COLLECTION,
                vectors_config=qmodels.VectorParams(
                    size=vector_size, 
                    distance=qmodels.Distance.COSINE
                ),
                # –£–±–∏—Ä–∞–µ–º default_segment_number=1.
                # –û—Å—Ç–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–º –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º. –≠—Ç–æ —Å–Ω–∏–∑–∏—Ç —Ä–∏—Å–∫ –∫–æ—Ä—Ä—É–ø—Ü–∏–∏ –ø—Ä–∏ —Å–±–æ—è—Ö.
                hnsw_config=qmodels.HnswConfigDiff(
                    m=16,
                    ef_construct=100,
                )
            )
            print("‚úÖ Collection created successfully")
            
        except Exception as e:
            print(f"‚ùå Collection setup failed: {e}")
            raise

    def close(self):
        loop = asyncio.get_event_loop()
        loop.run_until_complete(self.client.aclose())


# –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–∏—Å –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
service = QdrantService()

if __name__ == "__main__":
    service.run()
