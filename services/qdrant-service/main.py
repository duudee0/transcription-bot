# qdrant_service.py
import functools
import os
import uuid
import tempfile
import aiofiles
import httpx
import asyncio
import hashlib
from fastapi import HTTPException
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple

from common.base_service import BaseService
from common.models import PayloadType, TaskMessage, Data

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
try:
    from sentence_transformers import SentenceTransformer
except Exception:
    SentenceTransformer = None

try:
    from qdrant_client import QdrantClient
    from qdrant_client.http import models as qmodels
except Exception:
    QdrantClient = None
    qmodels = None

# –î–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è PDF (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
try:
    import fitz  # PyMuPDF
except Exception:
    fitz = None

# -------------------------
# –ö–æ–Ω—Ñ–∏–≥ –ø–æ –æ–∫—Ä—É–∂–µ–Ω–∏—é
# -------------------------
EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL", "sentence-transformers/all-MiniLM-L6-v2")
QDRANT_HOST = os.getenv("QDRANT_HOST", "127.0.0.1")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY", "")
QDRANT_COLLECTION = os.getenv("QDRANT_COLLECTION", "documents")
EMBEDDING_BATCH_SIZE = int(os.getenv("EMBEDDING_BATCH_SIZE", "32"))
CHUNK_SIZE_CHARS = int(os.getenv("CHUNK_SIZE_CHARS", "3000"))
CHUNK_OVERLAP_CHARS = int(os.getenv("CHUNK_OVERLAP_CHARS", "500"))
MAX_DOWNLOAD_SIZE = int(os.getenv("MAX_DOWNLOAD_SIZE_BYTES", str(200 * 1024 * 1024)))  # 200MB
DOWNLOAD_TIMEOUT = int(os.getenv("DOWNLOAD_TIMEOUT", "60"))

# -------------------------
# –°–µ—Ä–≤–∏—Å
# -------------------------
class QdrantService(BaseService):
    """
    –°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Qdrant: —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤/–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ -> —á–∞–Ω–∫–∏–Ω–≥ -> —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ -> upsert –≤ Qdrant.
    –¢–∞–∫–∂–µ —É–º–µ–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É (embedding-based search) –∏ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏.
    """

    def __init__(self):
        super().__init__("qdrant-service", "1.0")

        # httpx –∫–ª–∏–µ–Ω—Ç (async) –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏ –≤—ã–∑–æ–≤–∞ –≤–Ω–µ—à–Ω–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
        self.client = httpx.AsyncClient(timeout=DOWNLOAD_TIMEOUT)

        # –ó–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (–ª–æ–∫–∞–ª—å–Ω–æ, blocking)
        if SentenceTransformer is None:
            raise RuntimeError("sentence-transformers is required but not installed. Install sentence-transformers.")
        print(f"üîÑ Loading embedding model: {EMBEDDING_MODEL_NAME}")
        self.embed_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
        print(f"‚úÖ Embedding model loaded: {EMBEDDING_MODEL_NAME}")

        # Qdrant client (sync) - –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ executor –¥–ª—è async –≤—ã–∑–æ–≤–æ–≤
        if QdrantClient is None:
            raise RuntimeError("qdrant-client is required but not installed. Install qdrant-client.")
        print(f"üîÑ Connecting to Qdrant at {QDRANT_HOST}:{QDRANT_PORT}")
        if QDRANT_API_KEY:
            self.qdrant = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT, api_key=QDRANT_API_KEY)
        else:
            self.qdrant = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
        # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        try:
            self._ensure_collection_sync()
            print(f"‚úÖ Qdrant collection '{QDRANT_COLLECTION}' ready")
        except Exception as e:
            print(f"‚ùå Failed to ensure Qdrant collection: {e}")
            raise

        # –õ–∏–º–∏—Ç—ã/–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.chunk_size = CHUNK_SIZE_CHARS
        self.chunk_overlap = CHUNK_OVERLAP_CHARS
        self.embedding_batch = EMBEDDING_BATCH_SIZE

        #! –¢–ï–°–¢ –≠–ú–ë–ï–î–ò–ù–ì–ê
        # print("\nüíæ TESTING EMBEDING MODEL ")
        # print(self.embed_model.encode(["hi","i"]))

    def _can_handle_task_type(self, task_type: str) -> bool:
        supported = [
            "index_document",
            "index_text",
            "search",
            "reindex_document",
        ]
        return task_type in supported

    def _health_handler(self):
        try:
            return {
                "status": "ok",
                "service": self.service_name,
                "qdrant": {"host": QDRANT_HOST, "port": QDRANT_PORT, "collection": QDRANT_COLLECTION},
                "embedding_model": EMBEDDING_MODEL_NAME
            }
        except Exception as e:
            return {"status": "error", "error": str(e)}

    async def _validate_task(self, task_message: TaskMessage):
        if task_message.data.payload_type == PayloadType.TEXT:
            text = task_message.data.payload.get("text", "")
            if not text or not text.strip():
                raise HTTPException(status_code=400, detail="Text is required for indexing")
            
        elif task_message.data.payload_type == PayloadType.FILE:
            # –æ–∂–∏–¥–∞–µ–º file_url
            file_url = task_message.data.payload.get("file_url", "")
            if not file_url:
                raise HTTPException(status_code=400, detail="file_url is required for index_document")
            
        else:
            raise HTTPException(status_code=400, detail="Unsupported task_type")

    # -------------------------
    # –õ–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á–∏ 
    # ------------------------- 
    async def _process_task_logic(self, task_message: TaskMessage) -> Data:
        task_type = task_message.data.task_type
        payload_type = task_message.data.payload_type
        if payload_type == PayloadType.FILE:
            return await self._handle_index_document(task_message)
        elif payload_type == PayloadType.TEXT and task_type == "index_text":
            return await self._handle_index_text(task_message)
        elif payload_type == PayloadType.TEXT:
            return await self._handle_search(task_message)
        else:
            raise HTTPException(status_code=400, detail=f"Unknown task_type: {task_type} or/and no support type payload{payload_type}")

    # -------------------------
    # Indexing flows
    # -------------------------
    async def _handle_index_document(self, task_message: TaskMessage) -> Data:
        payload = task_message.data.payload
        file_url = payload.get("file_url")
        owner = payload.get("owner", "unknown")

        # —Å–∫–∞—á–∏–≤–∞–µ–º –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
        temp_path = await self._download_file(file_url)
        try:
            text = await self._extract_text_from_file(temp_path)

        finally:
            # —É–¥–∞–ª—è–µ–º —Ñ–∞–π–ª
            if os.path.exists(temp_path):
                os.unlink(temp_path)

        if not text or not text.strip():
            raise HTTPException(status_code=400, detail="No text extracted from document")

        # –ß–∞–Ω–∫–∏–Ω–≥
        chunks = self._chunk_text(text, chunk_size=self.chunk_size, overlap=self.chunk_overlap)

        # –°–æ–∑–¥–∞—ë–º –º–µ—Ç–∞ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
        doc_id = payload.get("doc_id") or f"doc-{uuid.uuid4().hex}"
        # –≤—ã—á–∏—Å–ª—è–µ–º checksum –≤—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–∞
        checksum = hashlib.sha256(text.encode("utf-8")).hexdigest()

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–æ—á–µ–∫ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        points = []
        for idx, (chunk_text, start_offset) in enumerate(chunks):
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —á–∏—Ç–∞–µ–º—ã–π id, –Ω–æ –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –∫–∞–∫ point id –≤ Qdrant
            original_chunk_id = f"{doc_id}::chunk::{idx}::{uuid.uuid4().hex}"
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∞–ª–∏–¥–Ω—ã–π –¥–ª—è Qdrant id (UUID string)
            qdrant_point_id = str(uuid.uuid4())

            # –ü—Ä–∏–≤–æ–¥–∏–º —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞ –∫ —Å—Ç—Ä–æ–∫–µ (—Å—Ç—Ä–∞—Ö –æ—Ç –Ω–µ—Å—Ç—Ä–æ–∫)
            chunk_text_str = chunk_text if isinstance(chunk_text, str) else str(chunk_text)

            payload_meta = {
                "doc_id": doc_id,
                "owner": owner,
                "offset": start_offset,
                "chunk_index": idx,
                "origin_url": file_url,
                "checksum": checksum,
                "text_preview": chunk_text_str[:500],
                "source_id": original_chunk_id  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º —á–∏—Ç–∞–±–µ–ª—å–Ω—ã–π id
            }
            points.append({"id": qdrant_point_id, "text": chunk_text_str, "payload": payload_meta})

        # –ü–æ–ª—É—á–∞–µ–º embeddings –±–∞—Ç—á–∞–º–∏ (blocking)
        embeddings = await self._embed_texts([p["text"] for p in points])

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø–∏—Å—å –¥–ª—è Qdrant (id —É–∂–µ –≤–∞–ª–∏–¥–Ω—ã)
        q_points = []
        for p, emb in zip(points, embeddings):
            vec = emb.tolist() if hasattr(emb, "tolist") else list(emb)
            q_points.append({"id": p["id"], "vector": vec, "payload": p["payload"]})

        # Upsert –≤ Qdrant
        upserted = await self._qdrant_upsert(q_points)

        return Data(
            payload_type=PayloadType.TEXT,
            task_type="index_document",
            payload={
                "task": "index_document",
                "doc_id": doc_id,
                "chunks_indexed": len(q_points),
                "upsert_result": upserted
            },
            execution_metadata={"service": self.service_name}
        )


    async def _handle_index_text(self, task_message: TaskMessage) -> Data:
        payload = task_message.data.payload
        text = payload.get("text", "")
        owner = payload.get("owner", "unknown")
        doc_id = payload.get("doc_id") or f"doc-{uuid.uuid4().hex}"

        chunks = self._chunk_text(text, chunk_size=self.chunk_size, overlap=self.chunk_overlap)

        points = []
        for idx, (chunk_text, start_offset) in enumerate(chunks):
            original_chunk_id = f"{doc_id}::chunk::{idx}::{uuid.uuid4().hex}"
            qdrant_point_id = str(uuid.uuid4())
            chunk_text_str = chunk_text if isinstance(chunk_text, str) else str(chunk_text)

            payload_meta = {
                "doc_id": doc_id,
                "owner": owner,
                "offset": start_offset,
                "chunk_index": idx,
                "text_preview": chunk_text_str[:500],
                "source_id": original_chunk_id
            }
            points.append({"id": qdrant_point_id, "text": chunk_text_str, "payload": payload_meta})

        embeddings = await self._embed_texts([p["text"] for p in points])

        q_points = []
        for p, emb in zip(points, embeddings):
            vec = emb.tolist() if hasattr(emb, "tolist") else list(emb)
            q_points.append({"id": p["id"], "vector": vec, "payload": p["payload"]})

        upserted = await self._qdrant_upsert(q_points)

        return Data(
            payload_type=PayloadType.TEXT,
            task_type="index_text",
            payload={
                "task": "index_text",
                "doc_id": doc_id,
                "chunks_indexed": len(q_points),
                "upsert_result": upserted
            },
            execution_metadata={"service": self.service_name}
        )

    # -------------------------
    # Search flow
    # -------------------------
    async def _handle_search(self, task_message: TaskMessage) -> Data:
        payload = task_message.data.payload
        query = payload.get("text", "").strip()  # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ .strip()!
        top_k = int(payload.get("top_k", 6))
        
        # === –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: –ó–ê–ü–†–ï–©–ê–ï–ú –ü–£–°–¢–´–ï –ó–ê–ü–†–û–°–´ ===
        if not query:
            raise HTTPException(
                status_code=400,
                detail="Search query cannot be empty. Please provide a meaningful question."
            )
        
        print(f"üîç Processing search query: '{query}'")  # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏
        
        # –ü–æ–ª—É—á–∞–µ–º embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        q_embs = await self._embed_texts([query])
        q_emb = q_embs[0]
        
        # === –í–ê–õ–ò–î–ê–¶–ò–Ø –≠–ú–ë–ï–î–î–ò–ù–ì–ê ===
        if not self._is_valid_embedding(q_emb):
            raise HTTPException(
                status_code=400,
                detail="Failed to generate meaningful embedding for the query. Try rephrasing."
            )
        
        # –ü–æ–∏—Å–∫ –≤ Qdrant —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É —Å–∫–æ—Ä—É
        search_results = await self._qdrant_search(
            vector=q_emb, 
            top=top_k,
            score_threshold=0.3  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–ª—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        )
        
        # –ï—Å–ª–∏ –Ω–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç —Å –ø–æ–¥—Å–∫–∞–∑–∫–æ–π
        if not search_results:
            return Data(
                payload_type=PayloadType.TEXT,
                task_type="search",
                payload={
                    "task": "search",
                    "query": query,
                    "results": [],
                    "message": "No relevant results found. Try rephrasing your question."
                },
                execution_metadata={"service": self.service_name}
            )
        
        return Data(
            payload_type=PayloadType.TEXT,
            task_type="search",
            payload={
                "task": "search",
                "query": query,
                "results": search_results
            },
            execution_metadata={"service": self.service_name}
        )

    def _is_valid_embedding(self, emb) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ —ç–º–±–µ–¥–¥–∏–Ω–≥ –Ω–µ –≤—ã—Ä–æ–∂–¥–µ–Ω–Ω—ã–π (–Ω–µ –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä)"""
        import numpy as np
        if hasattr(emb, "tolist"):
            emb = emb.tolist()
        return np.linalg.norm(emb) > 0.1  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≤–µ–∫—Ç–æ—Ä–∞

    # -------------------------
    # –£—Ç–∏–ª–∏—Ç—ã: —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ, —á–∞—Ä–Ω–∫–∏, —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, qdrant ops
    # -------------------------
    async def _download_file(self, url: str) -> str:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª –≤ temp –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å"""
        # –ü—Ä–æ—Å—Ç–∞—è –∑–∞—â–∏—Ç–∞: –∑–∞–ø—Ä–µ—â–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –∞–¥—Ä–µ—Å–∞ (SSRF)
        parsed = httpx.URL(url)
        if parsed.host in ("127.0.0.1", "localhost"):
            raise HTTPException(status_code=400, detail="Localhost downloads are forbidden")

        tmp_dir = tempfile.gettempdir()
        tmp_name = f"qdrant_{uuid.uuid4().hex}.pdf"
        tmp_path = os.path.join(tmp_dir, tmp_name)

        try:
            async with self.client.stream("GET", url, timeout=DOWNLOAD_TIMEOUT) as response:
                response.raise_for_status()
                total = 0
                async with aiofiles.open(tmp_path, "wb") as f:
                    async for chunk in response.aiter_bytes():
                        total += len(chunk)
                        if total > MAX_DOWNLOAD_SIZE:
                            await f.close()
                            raise HTTPException(status_code=400, detail="File too large")
                        await f.write(chunk)
        except httpx.RequestError as e:
            raise HTTPException(status_code=400, detail=f"Failed to download: {str(e)}")
        return tmp_path

    async def _extract_text_from_file(self, file_path: str) -> str:
        """–ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞: pdf -> text, txt -> decode"""
        # –ï—Å–ª–∏ PDF –∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω fitz (PyMuPDF)
        ext = Path(file_path).suffix.lower()
        if ext in (".pdf",) and fitz is not None:
            # blocking -> run in executor
            loop = asyncio.get_event_loop()
            text = await loop.run_in_executor(None, self._sync_extract_pdf_text, file_path)
            return text
        else:
            # –ø—Ä–æ–±—É–µ–º –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫–∞–∫ —Ç–µ–∫—Å—Ç
            # try:
            #     async with aiofiles.open(file_path, "rb") as f:
            #         data = await f.read()
            #         try:
            #             return data.decode("utf-8")
            #         except Exception:
            #             try:
            #                 return data.decode("latin-1")
            #             except Exception:
            #                 return ""
            # except Exception:
            return ""

    def _sync_extract_pdf_text(self, file_path: str) -> str:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ pdf —á–µ—Ä–µ–∑ PyMuPDF"""
        try:
            doc = fitz.open(file_path)
            parts = []
            for page in doc:
                parts.append(page.get_text("text"))
            return "\n".join(parts)
        except Exception:
            return ""

    def _chunk_text(self, text: str, chunk_size: int = 3000, overlap: int = 500) -> List[Tuple[str, int]]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ —Å–∏–º–≤–æ–ª–∞–º —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ (chunk_text, start_offset)
        """
        if not text:
            return []
        length = len(text)
        chunks = []
        start = 0
        while start < length:
            end = start + chunk_size
            chunk = text[start:end]
            chunks.append((chunk, start))
            if end >= length:
                break
            # —Å–¥–≤–∏–≥–∞–µ–º –Ω–∞ chunk_size - overlap
            start = end - overlap
        return chunks

    async def _embed_texts(self, texts: List[str]) -> List[Any]:
        """
        –ù–∞–¥—ë–∂–Ω—ã–π –±–∞—Ç—á–µ–≤—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥: –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–¥, –ª–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–±–ª–µ–º—É –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (numpy arrays / list).
        """
        # 1) –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Ö–æ–¥: –ø—Ä–∏–≤–æ–¥–∏–º –≤—Å—ë –∫ —Å—Ç—Ä–æ–∫–∞–º (–∏ –ª–æ–≥–∏—Ä—É–µ–º —Å–ª—É—á–∞–∏ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è)
        norm_texts = []
        for i, t in enumerate(texts):
            if isinstance(t, str):
                norm_texts.append(t)
            else:
                try:
                    s = str(t)
                    print("qdrant: embed_texts coerced input[%d] of type %s to str", i, type(t).__name__)
                    norm_texts.append(s)
                except Exception as e:
                    print("qdrant: cannot coerce input[%d] to str: %s", i, e)
                    raise RuntimeError(f"Invalid input type for embedding at index {i}: {type(t)}") from e

        loop = asyncio.get_event_loop()
        embeddings = []

        for i in range(0, len(norm_texts), self.embedding_batch):
            batch = norm_texts[i:i + self.embedding_batch]
            # –õ–æ–≥–∏—Ä—É–µ–º –∫—Ä–∞—Ç–∫–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –±–∞—Ç—á–∞ (–ø–µ—Ä–≤—ã–µ 3 —ç–ª–µ–º–µ–Ω—Ç–æ–≤) –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            print("qdrant: embedding batch start_idx=%d size=%d sample_types=%s", i, len(batch),
                        [type(x).__name__ for x in batch[:3]])

            # –í—ã–∑—ã–≤–∞–µ–º encode –≤ executor —Å –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–º–∏ –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏
            func = functools.partial(self.embed_model.encode,
                                    batch,
                                    convert_to_numpy=True,
                                    show_progress_bar=False)
            try:
                emb = await loop.run_in_executor(None, func)
                embeddings.extend(emb)
            except Exception as exc:
                # –ü–æ–¥—Ä–æ–±–Ω—ã–π –ª–æ–≥ –ø–µ—Ä–µ–¥ –ø–æ–ø—ã—Ç–∫–æ–π –ø–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ–≥–æ fallback
                print("qdrant: embedding failed on batch starting at %d: %s", i, exc)
                print("qdrant: problematic batch preview: %s", [repr(x)[:300] for x in batch[:10]])

                # fallback: –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —ç–º–±–µ–¥–¥–∏—Ç—å —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ –æ–¥–Ω–æ–º—É, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –ø—Ä–æ–±–ª–µ–º–Ω—ã–π
                for j, single in enumerate(batch):
                    try:
                        func_single = functools.partial(self.embed_model.encode,
                                                        [single],
                                                        convert_to_numpy=True,
                                                        show_progress_bar=False)
                        single_emb = await loop.run_in_executor(None, func_single)
                        embeddings.extend(single_emb)
                    except Exception as e2:
                        print("qdrant: single encode failed at global_index=%d: %s", i + j, e2)
                        # –ü–æ–¥–Ω–∏–º–∞–µ–º –ø–æ–Ω—è—Ç–Ω—É—é –æ—à–∏–±–∫—É —Å –∏–Ω–¥–µ–∫—Å–æ–º –ø—Ä–æ–±–ª–µ–º–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
                        raise RuntimeError(f"Embedding failed for item index {i + j}: type={type(single).__name__}, repr={repr(single)[:500]}") from e2
        return embeddings

    async def _qdrant_upsert(self, points: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Upsert points –≤ Qdrant (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç sync client –≤ executor).
        points: [{"id":..., "vector":[...], "payload": {...}}, ...]
        """
        loop = asyncio.get_event_loop()
        def _sync_upsert():
            # prepare qdrant points
            q_points = []
            for p in points:
                q_points.append(qmodels.PointStruct(id=p["id"], vector=p["vector"], payload=p["payload"]))
            self.qdrant.upsert(collection_name=QDRANT_COLLECTION, points=q_points)
            return {"upserted": len(q_points)}
        return await loop.run_in_executor(None, _sync_upsert)





    #! –¢–ï–°–¢–û–í–´–ô –ú–ï–¢–û–î –î–õ–Ø –ü–†–û–í–ï–†–ö–ò –í–°–ï–• –î–û–ö–£–ú–ï–ù–¢–û–í –ò–ó –ö–î–†–ê–ù–¢
    def _debug_get_all_documents(self, limit: int = 5):
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ doc_id
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º scroll –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö —Ç–æ—á–µ–∫
            all_points = []
            next_page = None
            while len(all_points) < limit * 10:  # –ë–µ—Ä–µ–º —Å –∑–∞–ø–∞—Å–æ–º
                points, next_page = self.qdrant.scroll(
                    collection_name=QDRANT_COLLECTION,
                    limit=100,
                    offset=next_page,
                    with_payload=["doc_id", "text_preview", "offset"]
                )
                all_points.extend(points)
                if next_page is None:
                    break
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ doc_id
            from collections import defaultdict
            docs = defaultdict(list)
            for point in all_points:
                doc_id = point.payload.get("doc_id")
                if doc_id:
                    docs[doc_id].append(point)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = []
            for doc_id, points in list(docs.items())[:limit]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                if not points:
                    continue
                    
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º —á–∞–Ω–∫–∏ –ø–æ –ø–æ–∑–∏—Ü–∏–∏
                chunks = sorted(points, key=lambda x: x.payload.get('offset', 0))
                last_chunk = chunks[-1]
                last_words = last_chunk.payload.get('text_preview', '').strip().split()[-3:]
                
                result.append({
                    "doc_id": doc_id,
                    "total_chunks": len(points),
                    "first_chunk_preview": chunks[0].payload.get('text_preview', '')[:50] + "...",
                    "last_chunk_preview": last_chunk.payload.get('text_preview', '')[:50] + "...",
                    "last_words": " ".join(last_words),
                    "total_characters": sum(len(p.payload.get('text_preview', '')) for p in points)
                })
            
            return result
            
        except Exception as e:
            print(f"‚ùå DEBUG ERROR: {str(e)}")
            return [{"error": str(e)}]




    async def _qdrant_search(self, vector: Any, top: int = 6, score_threshold: float = 0.3) -> List[Dict[str, Any]]:
        """
        –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –¥–ª—è qdrant-client >= 1.7.0
        """
        loop = asyncio.get_event_loop()

        def _sync_query():
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤–µ–∫—Ç–æ—Ä –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            q_vec = [float(x) for x in vector.tolist()] if hasattr(vector, "tolist") else [float(x) for x in vector]
            
            # === –ü–†–ê–í–ò–õ–¨–ù–´–ô –í–´–ó–û–í –î–õ–Ø –°–û–í–†–ï–ú–ï–ù–ù–û–ì–û QDRANT ===
            resp = self.qdrant.query_points(
                collection_name=QDRANT_COLLECTION,
                query=q_vec,  # ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û –î–õ–Ø query_points()
                using=None,   # ‚úÖ None –¥–ª—è default –≤–µ–∫—Ç–æ—Ä–∞ (–∏–ª–∏ –∏–º—è –µ—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–µ–∫—Ç–æ—Ä–æ–≤)
                limit=top,
                with_payload=True,
                score_threshold=0.3,  # ‚úÖ –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ü–û –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–ò
                with_vectors=False
            )
            
            # === –ü–†–ê–í–ò–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===
            hits = []
            for pt in resp.points:  # ‚úÖ resp.points –≤–º–µ—Å—Ç–æ resp.result
                # –û—Ç–ª–∞–¥–∫–∞: –ø–æ–∫–∞–∑—ã–≤–∞–µ–º preview –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                preview = pt.payload.get('text_preview', '')
                print(f"üîç Hit (score={pt.score}): {preview}...")
                
                hits.append({
                    "id": pt.id,
                    "score": pt.score,  # –î–ª—è –∫–æ—Å–∏–Ω—É—Å–∞: 0.0-1.0
                    "payload": pt.payload,
                })
            
            print(f"‚úÖ Found {len(hits)} relevant results (score_threshold=0.3)")
            # üî• –ù–û–í–´–ô –†–ï–ñ–ò–ú: –ü–†–û–í–ï–†–ö–ê –í–°–ï–• –î–û–ö–£–ú–ï–ù–¢–û–í
            print("\n" + "="*60)
            print("üêû DEBUG ALL DOCUMENTS (first 5)")
            print("="*60)
            docs = self._debug_get_all_documents(limit=5)
            
            for i, doc in enumerate(docs):
                print(f"\nüìÑ Document #{i+1}: {doc.get('doc_id', 'N/A')}")
                print(f"   Chunks: {doc.get('total_chunks', 'N/A')}")
                print(f"   First: '{doc.get('first_chunk_preview', '')}'")
                print(f"   Last:  '{doc.get('last_chunk_preview', '')}'")
                print(f"   Final words: '{doc.get('last_words', '')}'")
        
            print("="*60 + "\n")

            return hits

        return await loop.run_in_executor(None, _sync_query)
    
    def _ensure_collection_sync(self):
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Å–æ–∑–¥–∞—ë—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ init)"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        collections = self.qdrant.get_collections().collections
        names = [c.name for c in collections]
        if QDRANT_COLLECTION not in names:
            # —Å–æ–∑–¥–∞—ë–º collection —Å —Ä–∞–∑–º–µ—Ä–æ–º –≤–µ–∫—Ç–æ—Ä–∞ –∏–∑ –º–æ–¥–µ–ª–∏
            vector_size = self.embed_model.get_sentence_embedding_dimension()
            self.qdrant.create_collection(
                collection_name=QDRANT_COLLECTION,
                vectors_config=qmodels.VectorParams(size=vector_size, distance=qmodels.Distance.COSINE)
            )

    def close(self):
        loop = asyncio.get_event_loop()
        loop.run_until_complete(self.client.aclose())


# –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–∏—Å –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
service = QdrantService()

if __name__ == "__main__":
    service.run()
