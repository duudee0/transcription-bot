# qdrant_service.py
import functools
import os
import uuid
import tempfile
import aiofiles
import httpx
import asyncio
import hashlib
from fastapi import HTTPException
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple

from common.base_service import BaseService
from common.models import PayloadType, TaskMessage, Data

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
try:
    from sentence_transformers import SentenceTransformer
except Exception:
    SentenceTransformer = None

try:
    from qdrant_client import QdrantClient
    from qdrant_client.http import models as qmodels
except Exception:
    QdrantClient = None
    qmodels = None

# –î–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è PDF (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
try:
    import fitz  # PyMuPDF
except Exception:
    fitz = None

# -------------------------
# –ö–æ–Ω—Ñ–∏–≥ –ø–æ –æ–∫—Ä—É–∂–µ–Ω–∏—é
# -------------------------
EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL", "sentence-transformers/all-MiniLM-L6-v2")
QDRANT_HOST = os.getenv("QDRANT_HOST", "qdrant")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY", "")
QDRANT_COLLECTION = os.getenv("QDRANT_COLLECTION", "documents")
EMBEDDING_BATCH_SIZE = int(os.getenv("EMBEDDING_BATCH_SIZE", "32"))
CHUNK_SIZE_CHARS = int(os.getenv("CHUNK_SIZE_CHARS", "3000"))
CHUNK_OVERLAP_CHARS = int(os.getenv("CHUNK_OVERLAP_CHARS", "500"))
MAX_DOWNLOAD_SIZE = int(os.getenv("MAX_DOWNLOAD_SIZE_BYTES", str(200 * 1024 * 1024)))  # 200MB
DOWNLOAD_TIMEOUT = int(os.getenv("DOWNLOAD_TIMEOUT", "60"))

# -------------------------
# –°–µ—Ä–≤–∏—Å
# -------------------------
class QdrantService(BaseService):
    """
    –°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Qdrant: —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤/–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ -> —á–∞–Ω–∫–∏–Ω–≥ -> —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ -> upsert –≤ Qdrant.
    –¢–∞–∫–∂–µ —É–º–µ–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É (embedding-based search) –∏ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏.
    """

    def __init__(self):
        super().__init__("qdrant-service", "1.0")

        # httpx –∫–ª–∏–µ–Ω—Ç (async) –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏ –≤—ã–∑–æ–≤–∞ –≤–Ω–µ—à–Ω–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
        self.client = httpx.AsyncClient(timeout=DOWNLOAD_TIMEOUT)

        # –•—Ä–∞–Ω–µ–Ω–∏—è –∫–µ—à–∞ –º–æ–¥–µ–ª–∏
        cache_dir = os.getenv("MODEL_CACHE_DIR", "./model_cache")

        # –ó–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (–ª–æ–∫–∞–ª—å–Ω–æ, blocking)
        if SentenceTransformer is None:
            raise RuntimeError("sentence-transformers is required but not installed. Install sentence-transformers.")
        print(f"üîÑ Loading embedding model: {EMBEDDING_MODEL_NAME}")
        # –î–æ–±–∞–≤–ª—è–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç cache_folder
        self.embed_model = SentenceTransformer(
            EMBEDDING_MODEL_NAME, 
            cache_folder=cache_dir
        )
        print("‚úÖ Embedding model loaded")
        print(f"‚úÖ Embedding model loaded: {EMBEDDING_MODEL_NAME}")

        # Qdrant client (sync) - –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ executor –¥–ª—è async –≤—ã–∑–æ–≤–æ–≤
        if QdrantClient is None:
            raise RuntimeError("qdrant-client is required but not installed. Install qdrant-client.")
        print(f"üîÑ Connecting to Qdrant at {QDRANT_HOST}:{QDRANT_PORT}")
        if QDRANT_API_KEY:
            self.qdrant = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT, api_key=QDRANT_API_KEY)
        else:
            self.qdrant = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
        # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        try:
            self._ensure_collection_sync()
            print(f"‚úÖ Qdrant collection '{QDRANT_COLLECTION}' ready")
        except Exception as e:
            print(f"‚ùå Failed to ensure Qdrant collection: {e}")
            raise

        # –õ–∏–º–∏—Ç—ã/–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.chunk_size = CHUNK_SIZE_CHARS
        self.chunk_overlap = CHUNK_OVERLAP_CHARS
        self.embedding_batch = EMBEDDING_BATCH_SIZE

        #! –¢–ï–°–¢ –≠–ú–ë–ï–î–ò–ù–ì–ê
        # print("\nüíæ TESTING EMBEDING MODEL ")
        # print(self.embed_model.encode(["hi","i"]))

    def _can_handle_task_type(self, task_type: str) -> bool:
        supported = [
            "index_document",
            "index_text",
            "search",
            "reindex_document",
        ]
        return task_type in supported

    def _health_handler(self):
        try:
            return {
                "status": "ok",
                "service": self.service_name,
                "qdrant": {"host": QDRANT_HOST, "port": QDRANT_PORT, "collection": QDRANT_COLLECTION},
                "embedding_model": EMBEDDING_MODEL_NAME
            }
        except Exception as e:
            return {"status": "error", "error": str(e)}

    async def _validate_task(self, task_message: TaskMessage):
        if task_message.data.payload_type == PayloadType.TEXT:
            text = task_message.data.payload.get("text", "")
            if not text or not text.strip():
                raise HTTPException(status_code=400, detail="Text is required for indexing")
            
        elif task_message.data.payload_type == PayloadType.FILE:
            # –æ–∂–∏–¥–∞–µ–º file_url
            file_url = task_message.data.payload.get("file_url", "")
            if not file_url:
                raise HTTPException(status_code=400, detail="file_url is required for index_document")
            
        else:
            raise HTTPException(status_code=400, detail="Unsupported task_type")

    # -------------------------
    # –õ–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á–∏ 
    # ------------------------- 
    async def _process_task_logic(self, task_message: TaskMessage) -> Data:
        task_type = task_message.data.task_type
        payload_type = task_message.data.payload_type
        if payload_type == PayloadType.FILE:
            return await self._handle_index_document(task_message)
        elif payload_type == PayloadType.TEXT and task_type == "index_text":
            return await self._handle_index_text(task_message)
        elif payload_type == PayloadType.TEXT:
            return await self._handle_search(task_message)
        else:
            raise HTTPException(status_code=400, detail=f"Unknown task_type: {task_type} or/and no support type payload{payload_type}")

    # ---------------------------------------------------------
    # –û–±—Ä–∞–±–æ—Ç–∫–∞
    # ---------------------------------------------------------
    async def _process_text_content(self, text: str, doc_id: str, owner: str, origin_url: Optional[str] = None) -> Dict[str, Any]:
        """–û–±—â–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞: —á–∞–Ω–∫–∏–Ω–≥ -> —ç–º–±–µ–¥–¥–∏–Ω–≥ -> upsert"""
        if not text or not text.strip():
             raise HTTPException(status_code=400, detail="Empty text content")

        # –ß–∞–Ω–∫–∏–Ω–≥
        chunks = self._chunk_text(text, chunk_size=self.chunk_size, overlap=self.chunk_overlap)
        checksum = hashlib.sha256(text.encode("utf-8")).hexdigest()
        
        points = []
        previews = [] # –î–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Å–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç—ã –æ—Ç–¥–µ–ª—å–Ω–æ
        
        for idx, (chunk_text, start_offset) in enumerate(chunks):
            # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤
            chunk_text_str = str(chunk_text)
            
            qdrant_point_id = str(uuid.uuid4())
            original_chunk_id = f"{doc_id}::chunk::{idx}::{uuid.uuid4().hex}"

            payload_meta = {
                "doc_id": doc_id,
                "owner": owner,
                "offset": start_offset,
                "chunk_index": idx,
                "checksum": checksum,
                "text_preview": chunk_text_str[:500], # Magic number -> constant
                "source_id": original_chunk_id
            }
            if origin_url:
                payload_meta["origin_url"] = origin_url

            points.append({"id": qdrant_point_id, "text": chunk_text_str, "payload": payload_meta})
            previews.append(chunk_text_str)

        # –≠–º–±–µ–¥–¥–∏–Ω–≥ (batch processing)
        embeddings = await self._embed_texts(previews)

        # –°–±–æ—Ä–∫–∞ –¥–ª—è Qdrant
        q_points = []
        for p, emb in zip(points, embeddings):
            vec = emb.tolist() if hasattr(emb, "tolist") else list(emb)
            q_points.append({"id": p["id"], "vector": vec, "payload": p["payload"]})

        # Upsert
        upsert_result = await self._qdrant_upsert(q_points)
        
        return {
            "doc_id": doc_id,
            "chunks_count": len(q_points),
            "upsert_result": upsert_result
        }

    # ---------------------------------------------------------
    # Refactored Handlers
    # ---------------------------------------------------------
    async def _handle_index_document(self, task_message: TaskMessage) -> Data:
        payload = task_message.data.payload
        file_url = payload.get("file_url")
        owner = payload.get("owner", "unknown")
        doc_id = payload.get("doc_id") or f"doc-{uuid.uuid4().hex}"

        temp_path = await self._download_file(file_url)
        try:
            text = await self._extract_text_from_file(temp_path)
        finally:
            if os.path.exists(temp_path):
                os.unlink(temp_path)

        result = await self._process_text_content(text, doc_id, owner, origin_url=file_url)

        return Data(
            payload_type=PayloadType.TEXT,
            task_type="index_document",
            payload={**result, "task": "index_document"},
            execution_metadata={"service": self.service_name}
        )

    async def _handle_index_text(self, task_message: TaskMessage) -> Data:
        payload = task_message.data.payload
        text = payload.get("text", "")
        owner = payload.get("owner", "unknown")
        doc_id = payload.get("doc_id") or f"doc-{uuid.uuid4().hex}"

        result = await self._process_text_content(text, doc_id, owner)

        return Data(
            payload_type=PayloadType.TEXT,
            task_type="index_text",
            payload={**result, "task": "index_text"},
            execution_metadata={"service": self.service_name}
        )


    # -------------------------
    # Search flow
    # -------------------------
    async def _handle_search(self, task_message: TaskMessage) -> Data:
        payload = task_message.data.payload
        query = payload.get("text", "").strip()
        top_k = int(payload.get("top_k", 3))

        # 1. –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞
        if not query:
            raise HTTPException(status_code=400, detail="Search query cannot be empty.")

        # 2. –≠–º–±–µ–¥–¥–∏–Ω–≥
        try:
            q_embs = await self._embed_texts([query])
            q_emb = q_embs[0]
        except Exception as e:
             raise HTTPException(status_code=500, detail=f"Embedding generation failed: {str(e)}")

        if not self._is_valid_embedding(q_emb):
             raise HTTPException(status_code=400, detail="Generated embedding is invalid (zero vector).")

        # 3. –ü–æ–∏—Å–∫ –≤ Qdrant
        search_results = await self._qdrant_search(
            vector=q_emb,
            top=top_k,
            score_threshold=0.3
        )

        # 4. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ (—Ç–µ–∫—Å—Ç + –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)
        formatted_text, sources_meta = self._format_results(search_results, query)
        
        # –õ–æ–≥–∏–∫–∞ –æ—Ç–≤–µ—Ç–∞: –¥–∞–∂–µ –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º Data, –Ω–æ —Å –ø—É—Å—Ç—ã–º —Ç–µ–∫—Å—Ç–æ–º
        # –∏–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ–º, —á—Ç–æ–±—ã –ø–∞–π–ø–ª–∞–π–Ω –Ω–µ –ø–∞–¥–∞–ª —Å –æ—à–∏–±–∫–æ–π, –∞ LLM –∑–Ω–∞–ª–∞, —á—Ç–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–µ—Ç.
        
        final_payload = {
            "text": formatted_text if search_results else "No relevant context found.",
            "query": query, # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            "found_count": len(search_results)
        }

        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ execution_metadata
        exec_meta = {
            "service": self.service_name,
            "model": EMBEDDING_MODEL_NAME,
            "sources": sources_meta # –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–µ—Ç–∞–ª—è–º–∏ (url, doc_id, score)
        }

        return Data(
            payload_type=PayloadType.TEXT,
            task_type="search_result",
            payload=final_payload,
            execution_metadata=exec_meta
        )

    def _format_results(self, results: List[Dict[str, Any]], query: str) -> Tuple[str, List[Dict[str, Any]]]:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è LLM: —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        """
        if not results:
            return (
                f"No relevant documents found for query: '{query}'. "
                "Possible reasons: documents not indexed, query too specific, or low relevance threshold.",
                []
            )

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è LLM
        context_parts = []
        sources_metadata = []

        # –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è —Å—Å—ã–ª–æ–∫
        sources_index = {}
        for i, hit in enumerate(results):
            payload = hit["payload"]
            source_key = f"{payload.get('doc_id')}_{payload.get('chunk_index')}"
            
            if source_key not in sources_index:
                sources_index[source_key] = {
                    "id": len(sources_index) + 1,
                    "url": payload.get("origin_url"),
                    "doc_id": payload.get("doc_id"),
                    "owner": payload.get("owner")
                }
            
            sources_metadata.append({
                "source_id": sources_index[source_key]["id"],
                "score": round(hit["score"], 4),
                "text_preview": payload.get("text_preview", "")[:200],
                "chunk_index": payload.get("chunk_index")
            })

        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context_parts.append(f"Found {len(results)} relevant fragments for query: '{query}'")
        context_parts.append("\nSources:")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        for source in sources_index.values():
            source_ref = f"[{source['id']}]"
            if source["url"]:
                source_ref += f" URL: {source['url']}"
            else:
                source_ref += f" Document ID: {source['doc_id']}"
            source_ref += f" (Owner: {source['owner']})"
            context_parts.append(source_ref)
        
        context_parts.append("\nRelevant content fragments:")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Å —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        for i, hit in enumerate(results):
            payload = hit["payload"]
            source_key = f"{payload.get('doc_id')}_{payload.get('chunk_index')}"
            source_id = sources_index[source_key]["id"]
            
            fragment = (
                f"Fragment #{i+1} (Relevance: {hit['score']:.3f}, Source: [{source_id}]):\n"
                f"{payload.get('text_preview', '').strip()}"
            )
            context_parts.append(fragment)

        final_text = "\n".join(context_parts)
        return final_text, sources_metadata

    def _is_valid_embedding(self, emb) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ —ç–º–±–µ–¥–¥–∏–Ω–≥ –Ω–µ –≤—ã—Ä–æ–∂–¥–µ–Ω–Ω—ã–π (–Ω–µ –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä)"""
        import numpy as np
        if hasattr(emb, "tolist"):
            emb = emb.tolist()
        return np.linalg.norm(emb) > 0.1  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≤–µ–∫—Ç–æ—Ä–∞

    # -------------------------
    # –£—Ç–∏–ª–∏—Ç—ã
    # -------------------------
    async def _download_file(self, url: str) -> str:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª –≤ temp –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å"""
        # –ü—Ä–æ—Å—Ç–∞—è –∑–∞—â–∏—Ç–∞: –∑–∞–ø—Ä–µ—â–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –∞–¥—Ä–µ—Å–∞ (SSRF)
        parsed = httpx.URL(url)
        if parsed.host in ("127.0.0.1", "localhost"):
            raise HTTPException(status_code=400, detail="Localhost downloads are forbidden")

        tmp_dir = tempfile.gettempdir()
        tmp_name = f"qdrant_{uuid.uuid4().hex}.pdf"
        tmp_path = os.path.join(tmp_dir, tmp_name)

        try:
            async with self.client.stream("GET", url, timeout=DOWNLOAD_TIMEOUT) as response:
                response.raise_for_status()
                total = 0
                async with aiofiles.open(tmp_path, "wb") as f:
                    async for chunk in response.aiter_bytes():
                        total += len(chunk)
                        if total > MAX_DOWNLOAD_SIZE:
                            await f.close()
                            raise HTTPException(status_code=400, detail="File too large")
                        await f.write(chunk)
        except httpx.RequestError as e:
            raise HTTPException(status_code=400, detail=f"Failed to download: {str(e)}")
        return tmp_path

    #TODO: –î–æ–±–∞–≤–∏—Ç—å —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã —Ñ–∞–π–ª–æ–≤
    async def _extract_text_from_file(self, file_path: str) -> str:
        """–ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞: pdf -> text, txt -> decode"""
        # –ï—Å–ª–∏ PDF –∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω fitz (PyMuPDF)
        ext = Path(file_path).suffix.lower()
        if ext in (".pdf",) and fitz is not None:
            # blocking -> run in executor
            loop = asyncio.get_event_loop()
            text = await loop.run_in_executor(None, self._sync_extract_pdf_text, file_path)
            return text
        else:
            # –ø—Ä–æ–±—É–µ–º –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫–∞–∫ —Ç–µ–∫—Å—Ç
            # try:
            #     async with aiofiles.open(file_path, "rb") as f:
            #         data = await f.read()
            #         try:
            #             return data.decode("utf-8")
            #         except Exception:
            #             try:
            #                 return data.decode("latin-1")
            #             except Exception:
            #                 return ""
            # except Exception:
            return ""

    def _sync_extract_pdf_text(self, file_path: str) -> str:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ pdf —á–µ—Ä–µ–∑ PyMuPDF"""
        try:
            doc = fitz.open(file_path)
            parts = []
            for page in doc:
                parts.append(page.get_text("text"))
            return "\n".join(parts)
        except Exception:
            return ""

    def _chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[Tuple[str, int]]:
        """
        –£–º–Ω–∞—è –Ω–∞—Ä–µ–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞. 
        """
        if not text:
            return []

        chunks = []
        start = 0
        length = len(text)

        while start < length:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω–µ—Ü
            target_end = min(start + chunk_size, length)
            
            # –ò—â–µ–º, –≥–¥–µ –ª—É—á—à–µ –≤—Å–µ–≥–æ –æ–±—Ä–µ–∑–∞—Ç—å, –µ—Å–ª–∏ –º—ã –Ω–µ –≤ –∫–æ–Ω—Ü–µ —Ç–µ–∫—Å—Ç–∞
            end = target_end
            if end < length:
                end = self._find_smart_split_point(text, start, target_end, chunk_size)

            # –§–æ—Ä–º–∏—Ä—É–µ–º —á–∞–Ω–∫
            chunk = text[start:end].strip()
            if chunk:
                chunks.append((chunk, start))
            
            if end >= length:
                break
                
            # –°–¥–≤–∏–≥–∞–µ–º start –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —á–∞–Ω–∫–∞
            start = max(start + 1, end - overlap)

        return chunks

    def _find_smart_split_point(self, text: str, start: int, end: int, chunk_size: int) -> int:
        """
        –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥: –∏—â–µ—Ç –ª—É—á—à–∏–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –≤ –∫–æ–Ω—Ü–µ –æ—Ç—Ä–µ–∑–∫–∞.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –∫–æ–Ω—Ü–∞ —á–∞–Ω–∫–∞.
        """
        # –ó–æ–Ω–∞ –ø–æ–∏—Å–∫–∞: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 20% —á–∞–Ω–∫–∞
        search_start = max(start, end - int(chunk_size * 0.2))
        
        separators = ["\n\n", "\n", ". ", "! ", "? ", " "]
        
        for sep in separators:
            sep_pos = text.rfind(sep, search_start, end)
            if sep_pos != -1:
                return sep_pos + len(sep)
        
        # –ï—Å–ª–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–Ω–µ—Ü (—Ä–µ–∂–µ–º –∂–µ—Å—Ç–∫–æ)
        return end

    #
    async def _embed_texts(self, texts: List[str]) -> List[Any]:
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞. –ö–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å.
        """
        norm_texts = self._normalize_inputs(texts)
        loop = asyncio.get_event_loop()
        embeddings = []

        for i in range(0, len(norm_texts), self.embedding_batch):
            batch = norm_texts[i : i + self.embedding_batch]
            batch_embeddings = await self._process_embedding_batch(loop, batch, i)
            embeddings.extend(batch_embeddings)
            
        return embeddings

    def _normalize_inputs(self, texts: List[Any]) -> List[str]:
        """–ü—Ä–∏–≤–æ–¥–∏—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫ —Å—Ç—Ä–æ–∫–∞–º."""
        norm_texts = []
        for i, t in enumerate(texts):
            if isinstance(t, str):
                norm_texts.append(t)
            else:
                try:
                    # –õ—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å logger –≤–º–µ—Å—Ç–æ print
                    print(f"qdrant: coercing input[{i}] type {type(t).__name__} to str")
                    norm_texts.append(str(t))
                except Exception as e:
                    raise RuntimeError(f"Invalid input for embedding at index {i}: {e}")
        return norm_texts

    async def _process_embedding_batch(self, loop, batch: List[str], start_idx: int) -> List[Any]:
        """
        –ü—ã—Ç–∞–µ—Ç—Å—è –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –±–∞—Ç—á —Ü–µ–ª–∏–∫–æ–º. –ü—Ä–∏ –æ—à–∏–±–∫–µ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç –∫ –ø–æ—à—Ç—É—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ.
        """
        # –ß–∞—Å—Ç–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –≤ executor
        func = functools.partial(
            self.embed_model.encode, 
            batch, 
            convert_to_numpy=True, 
            show_progress_bar=False
        )

        try:
            return await loop.run_in_executor(None, func)
        except Exception as e:
            print(f"Batch embedding failed at idx {start_idx}: {e}. Switching to fallback.")
            return await self._process_batch_fallback(loop, batch, start_idx)

    async def _process_batch_fallback(self, loop, batch: List[str], start_idx: int) -> List[Any]:
        """
        –ú–µ–¥–ª–µ–Ω–Ω—ã–π —Ä–µ–∂–∏–º: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ –æ–¥–Ω–æ–º—É, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –±–∏—Ç—ã–π —ç–ª–µ–º–µ–Ω—Ç.
        """
        results = []
        for j, item in enumerate(batch):
            func_single = functools.partial(
                self.embed_model.encode, 
                [item], # encode –æ–∂–∏–¥–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∏–ª–∏ —Å—Ç—Ä–æ–∫—É, –Ω–æ –¥–ª—è consistency –ø–µ—Ä–µ–¥–∞–µ–º —Å–ø–∏—Å–æ–∫
                convert_to_numpy=True, 
                show_progress_bar=False
            )
            try:
                # –†–µ–∑—É–ª—å—Ç–∞—Ç encode –¥–ª—è —Å–ø–∏—Å–∫–∞ ‚Äî —ç—Ç–æ —Å–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤. –ë–µ—Ä–µ–º [0] –∏–ª–∏ extend
                emb = await loop.run_in_executor(None, func_single)
                results.extend(emb)
            except Exception as e:
                global_idx = start_idx + j
                print(f"Single encode failed at index {global_idx}")
                raise RuntimeError(f"Embedding failed for item {global_idx} (len={len(item)})") from e
        return results
    
    def _sync_upsert_safe(self, points_batch: List[Dict]) -> int:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π upsert —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏—Å–∫–ª—é—á–µ–Ω–∏–π"""
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–æ—á–µ–∫ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Qdrant 1.16.0
            q_points = []
            for p in points_batch:
                # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞
                vector = p["vector"]
                if hasattr(vector, "tolist"):
                    vector = vector.tolist()
                elif not isinstance(vector, list):
                    vector = [float(x) for x in vector]
                
                # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ—á–∫–∏ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
                q_points.append(qmodels.PointStruct(
                    id=str(p["id"]),  # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ ID —Å—Ç—Ä–æ–∫–æ–≤—ã–π
                    vector=vector,
                    payload=p.get("payload", {})
                ))
            
            print(f"üì§ Upserting {len(q_points)} points to collection '{QDRANT_COLLECTION}'")
            
            # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ upsert –¥–ª—è Qdrant 1.16.0
            self.qdrant.upsert(
                collection_name=QDRANT_COLLECTION,
                points=q_points,
                wait=True
            )
            
            print(f"‚úÖ Upsert successful: {len(q_points)} points")
            return len(q_points)
                
        except Exception as e:
            print(f"‚ùå Upsert failed: {str(e)}")
            import traceback
            traceback.print_exc()
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ç–ª–∞–¥–∫–∞ –æ—à–∏–±–∫–∏
            if "wrong input data" in str(e).lower() or "vectors" in str(e).lower():
                print("üîç Vector validation details:")
                for i, p in enumerate(points_batch[:3]):
                    vec = p["vector"]
                    print(f"  Point #{i}:")
                    print(f"    ID: {p.get('id')}")
                    print(f"    Vector type: {type(vec)}")
                    print(f"    Vector length: {len(vec) if hasattr(vec, '__len__') else 'unknown'}")
                    if hasattr(vec, "shape"):
                        print(f"    Vector shape: {vec.shape}")
            
            raise

    async def _upsert_points_one_by_one(self, loop, points: List[Dict]) -> int:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥: upsert –ø–æ –æ–¥–Ω–æ–π —Ç–æ—á–∫–µ"""
        success_count = 0
        for point in points:
            try:
                result = await loop.run_in_executor(
                    None,
                    functools.partial(self._sync_upsert_safe, [point])
                )
                success_count += result
            except Exception as e:
                print(f"‚ùå Single point upsert failed for {point.get('id')}: {e}")
        return success_count
 
    async def _qdrant_upsert(self, points: List[Dict[str, Any]], batch_size: int = 100) -> Dict[str, Any]:
        """Upsert —Å –ø–æ–ª–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö"""
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–æ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö
        validated_points = []
        for i, point in enumerate(points):
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
                if not point.get("id"):
                    point["id"] = str(uuid.uuid4())
                    
                if not point.get("vector"):
                    print(f"‚ö†Ô∏è Skipping point without vector: {point.get('id')}")
                    continue
                    
                if not isinstance(point["vector"], list) or len(point["vector"]) == 0:
                    print(f"‚ö†Ô∏è Skipping point with invalid vector: {point.get('id')}")
                    continue
                    
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞
                expected_size = self.embed_model.get_sentence_embedding_dimension()
                if len(point["vector"]) != expected_size:
                    print(f"‚ö†Ô∏è Vector size mismatch for point {point.get('id')}: {len(point['vector'])} != {expected_size}")
                    continue
                    
                validated_points.append(point)
                
            except Exception as e:
                print(f"‚ùå Point validation failed at index {i}: {e}")
                continue

        if not validated_points:
            return {"upserted": 0, "error": "No valid points to upsert"}

        print(f"‚úÖ Validated {len(validated_points)}/{len(points)} points for upsert")

        loop = asyncio.get_event_loop()
        success_count = 0

        for i in range(0, len(validated_points), batch_size):
            batch = validated_points[i:i + batch_size]
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–Ω—å—à–∏–π –±–∞—Ç—á –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
                result = await loop.run_in_executor(
                    None, 
                    functools.partial(self._sync_upsert_safe, batch)
                )
                success_count += result
                print(f"‚úÖ Successfully upserted batch {i//batch_size + 1}: {result} points")
                
                # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
                await asyncio.sleep(0.1)
                
            except Exception as e:
                print(f"‚ùå Batch upsert failed at index {i}: {e}")
                # –ü—Ä–æ–±—É–µ–º upsert –ø–æ –æ–¥–Ω–æ–º—É
                single_success = await self._upsert_points_one_by_one(loop, batch)
                success_count += single_success

        return {"upserted": success_count}


    #! –¢–ï–°–¢–û–í–´–ô –ú–ï–¢–û–î –î–õ–Ø –ü–†–û–í–ï–†–ö–ò –í–°–ï–• –î–û–ö–£–ú–ï–ù–¢–û–í –ò–ó –ö–î–†–ê–ù–¢
    def _debug_get_all_documents(self, limit: int = 5):
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ doc_id
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º scroll –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö —Ç–æ—á–µ–∫
            all_points = []
            next_page = None
            while len(all_points) < limit * 10:  # –ë–µ—Ä–µ–º —Å –∑–∞–ø–∞—Å–æ–º
                scroll_result = self.qdrant.scroll(...)
                points = scroll_result.points
                next_page = scroll_result.next_page_offset
                all_points.extend(points)
                if next_page is None:
                    break
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ doc_id
            from collections import defaultdict
            docs = defaultdict(list)
            for point in all_points:
                doc_id = point.payload.get("doc_id")
                if doc_id:
                    docs[doc_id].append(point)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = []
            for doc_id, points in list(docs.items())[:limit]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                if not points:
                    continue
                    
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º —á–∞–Ω–∫–∏ –ø–æ –ø–æ–∑–∏—Ü–∏–∏
                chunks = sorted(points, key=lambda x: x.payload.get('offset', 0))
                last_chunk = chunks[-1]
                last_words = last_chunk.payload.get('text_preview', '').strip().split()[-3:]
                
                result.append({
                    "doc_id": doc_id,
                    "total_chunks": len(points),
                    "first_chunk_preview": chunks[0].payload.get('text_preview', '')[:50] + "...",
                    "last_chunk_preview": last_chunk.payload.get('text_preview', '')[:50] + "...",
                    "last_words": " ".join(last_words),
                    "total_characters": sum(len(p.payload.get('text_preview', '')) for p in points)
                })
            
            return result
            
        except Exception as e:
            print(f"‚ùå DEBUG ERROR: {str(e)}")
            return [{"error": str(e)}]




    async def _qdrant_search(self, vector: Any, top: int = 6, score_threshold: float = 0.3) -> List[Dict[str, Any]]:
        """
        –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –¥–ª—è qdrant-client >= 1.7.0
        """
        loop = asyncio.get_event_loop()

        def _sync_query():
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤–µ–∫—Ç–æ—Ä –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            q_vec = vector.tolist() if hasattr(vector, "tolist") else [float(x) for x in vector]
            
            # === –ü–†–ê–í–ò–õ–¨–ù–´–ô –í–´–ó–û–í –î–õ–Ø –°–û–í–†–ï–ú–ï–ù–ù–û–ì–û QDRANT ===
            resp = self.qdrant.query_points(
                collection_name=QDRANT_COLLECTION,
                query=q_vec,  
                using=None,   
                limit=top,
                with_payload=True,
                score_threshold=score_threshold,  # ‚úÖ –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ü–û –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–ò
                with_vectors=False
            )
            
            # === –ü–†–ê–í–ò–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===
            hits = []
            for point in resp.points:
                hits.append({
                    "id": str(point.id),
                    "score": float(point.score),
                    "payload": point.payload or {},
                })
                # –û—Ç–ª–∞–¥–∫–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                preview = point.payload.get('text_preview', '')[:100] if point.payload else ''
                print(f"  üìå Hit (score={point.score:.4f}): {preview}...")
            
            print(f"‚úÖ Found {len(hits)} relevant results (score_threshold={score_threshold})")

            # üî• –ù–û–í–´–ô –†–ï–ñ–ò–ú: –ü–†–û–í–ï–†–ö–ê –í–°–ï–• –î–û–ö–£–ú–ï–ù–¢–û–í
            print("\n" + "="*60)
            print("üêû DEBUG ALL DOCUMENTS (first 5)")
            print("="*60)
            docs = self._debug_get_all_documents(limit=5)
            
            for i, doc in enumerate(docs):
                print(f"\nüìÑ Document #{i+1}: {doc.get('doc_id', 'N/A')}")
                print(f"   Chunks: {doc.get('total_chunks', 'N/A')}")
                print(f"   First: '{doc.get('first_chunk_preview', '')}'")
                print(f"   Last:  '{doc.get('last_chunk_preview', '')}'")
                print(f"   Final words: '{doc.get('last_words', '')}'")
        
            print("="*60 + "\n")

            return hits

        return await loop.run_in_executor(None, _sync_query)
    
    def _ensure_collection_sync(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        try:
            try:
                self.qdrant.get_collection(collection_name=QDRANT_COLLECTION)
                print(f"‚úÖ Collection '{QDRANT_COLLECTION}' already exists")
                return
            except Exception as e:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ –æ—à–∏–±–∫–∞ "not found", –∏–Ω–∞—á–µ —Ä–µ–π–∑–∏–º
                if "not found" not in str(e).lower() and "404" not in str(e):
                    print(f"‚ö†Ô∏è Collection check warning: {str(e)}")

            vector_size = self.embed_model.get_sentence_embedding_dimension()
            print(f"üîÑ Creating collection '{QDRANT_COLLECTION}' with vector size {vector_size}")

            # –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
            self.qdrant.create_collection(
                collection_name=QDRANT_COLLECTION,
                vectors_config=qmodels.VectorParams(
                    size=vector_size, 
                    distance=qmodels.Distance.COSINE
                ),
                # –£–±–∏—Ä–∞–µ–º default_segment_number=1.
                # –û—Å—Ç–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–º –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º. –≠—Ç–æ —Å–Ω–∏–∑–∏—Ç —Ä–∏—Å–∫ –∫–æ—Ä—Ä—É–ø—Ü–∏–∏ –ø—Ä–∏ —Å–±–æ—è—Ö.
                hnsw_config=qmodels.HnswConfigDiff(
                    m=16,
                    ef_construct=100,
                )
            )
            print("‚úÖ Collection created successfully")
            
        except Exception as e:
            print(f"‚ùå Collection setup failed: {e}")
            raise

    def close(self):
        loop = asyncio.get_event_loop()
        loop.run_until_complete(self.client.aclose())


# –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–∏—Å –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
service = QdrantService()

if __name__ == "__main__":
    service.run()
