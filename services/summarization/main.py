from common.base_service import BaseService
from common.models import PayloadType, TaskMessage, Data
from fastapi import HTTPException
import os
import torch
from transformers import MBartTokenizer, MBartForConditionalGeneration
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import asyncio

class SummarizationService(BaseService):
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ"""
    
    def __init__(self):
        super().__init__("summarization-service", "1.0")
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        self.model_name = os.getenv("SUMMARIZATION_MODEL", "IlyaGusev/mbart_ru_sum_gazeta")
        self.max_input_length = int(os.getenv("MAX_INPUT_LENGTH", "600"))
        self.device = os.getenv("DEVICE", "cuda" if torch.cuda.is_available() else "cpu")
        
        print(f"üîÑ Loading summarization model: {self.model_name}")
        print(f"üì± Using device: {self.device}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        try:
            # –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            self.tokenizer = MBartTokenizer.from_pretrained(self.model_name)
            
            # –ó–∞—Ç–µ–º –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ–º –Ω–µ—Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö –≤–µ—Å–æ–≤
            self.model = MBartForConditionalGeneration.from_pretrained(
                self.model_name,
                ignore_mismatched_sizes=True  # –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–æ–≤–ø–∞–¥–∞—é—â–∏–µ —Ä–∞–∑–º–µ—Ä—ã
            )
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            self.model.resize_token_embeddings(len(self.tokenizer))
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
            if self.device == "cuda" and torch.cuda.is_available():
                self.model = self.model.cuda()
                print("‚úÖ Model moved to GPU")
            else:
                print("‚ÑπÔ∏è  Using CPU for inference")
                
            print(f"‚úÖ Summarization model {self.model_name} loaded successfully")
            
        except Exception as e:
            print(f"‚ùå Failed to load model: {str(e)}")
            raise

    def _can_handle_task_type(self, task_type: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –º–æ–∂–µ—Ç –ª–∏ —Å–µ—Ä–≤–∏—Å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–∏–ø –∑–∞–¥–∞—á–∏"""
        supported_task_types = [
            "summarize_text",
            "text_summarization", 
            "generate_summary",
            "abstractive_summarization"
        ]
        return task_type in supported_task_types

    def _health_handler(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
        try:
            cuda_available = torch.cuda.is_available()
            current_device = "cuda" if next(self.model.parameters()).is_cuda else "cpu"
            
            return {
                "status": "ok",
                "service": self.service_name,
                "model": self.model_name,
                "device": current_device,
                "cuda_available": cuda_available,
                "model_loaded": self.model is not None,
                "tokenizer_loaded": self.tokenizer is not None,
                "max_input_length": self.max_input_length
            }
        except Exception as e:
            return {
                "status": "error",
                "service": self.service_name,
                "error": str(e)
            }

    async def _validate_task(self, task_message: TaskMessage):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–¥–∞—á–∏ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
        if task_message.data.payload_type != PayloadType.TEXT:
            raise HTTPException(
                status_code=400, 
                detail=f"Unsupported payload type: {task_message.data.payload_type}. Expected TEXT"
            )
        
        text = task_message.data.payload.get("text", "")
        if not text or not text.strip():
            raise HTTPException(status_code=400, detail="Text is required for summarization")

    async def _process_task_logic(self, task_message: TaskMessage) -> Data:
        """–õ–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
        text = task_message.data.payload.get("text", "")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é
        summary = await self._generate_summary(text)
        
        return Data(
            payload_type=PayloadType.TEXT,
            payload={
                "task": "text_summarization",
                "original_text": text,
                "summary": summary,
                "model_used": self.model_name,
                "input_length": len(text),
                "summary_length": len(summary)
            },
            execution_metadata={
                "task_type": "summarize_text",
                "service": "summarization-service",
                "model": self.model_name,
                "device": self.device
            }
        )
    
    async def _generate_summary(self, text: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞"""
        try:
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ thread pool, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –±–ª–æ–∫–∏—Ä—É—é—â–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è
            loop = asyncio.get_event_loop()
            summary = await loop.run_in_executor(
                None, 
                self._sync_generate_summary, 
                text
            )
            return summary
            
        except Exception as e:
            raise HTTPException(
                status_code=500, 
                detail=f"Summarization failed: {str(e)}"
            )
    
    def _sync_generate_summary(self, text: str) -> str:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
        try:
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            input_ids = self.tokenizer(
                [text],
                max_length=self.max_input_length,
                truncation=True,
                return_tensors="pt",
            )["input_ids"]
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ GPU –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
            if self.device == "cuda" and torch.cuda.is_available():
                input_ids = input_ids.cuda()
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            output_ids = self.model.generate(
                input_ids=input_ids,
                no_repeat_ngram_size=4,
                num_beams=5,           # –£–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                length_penalty=2.0,     # –ü–æ–æ—â—Ä—è–µ—Ç –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ summary
                min_length=30,          # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ summary
                max_length=100,         # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ summary
                early_stopping=True
            )[0]
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            summary = self.tokenizer.decode(output_ids, skip_special_tokens=True)
            return summary.strip()
            
        except Exception as e:
            print(f"üö® Summarization error: {str(e)}")
            raise RuntimeError(f"Summarization failed: {str(e)}") from e


# –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–∏—Å
service = SummarizationService()

if __name__ == "__main__":
    service.run()