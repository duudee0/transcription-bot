#  docker compose up

services:
  rabbitmq:
    image: rabbitmq:management
    restart: always
    environment:
      RABBITMQ_DEFAULT_USER: guest
      RABBITMQ_DEFAULT_PASS: guest
      #- RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS=-rabbit disk_free_limit 2147483648
    volumes:
      - ./rabbitmq:/var/lib/rabbitmq
    ports:
      - "15672:15672"  # management UI
      - "5672:5672"
    networks:
      - internal

  # redis:
  #   image: redis:8.2.2
  #   ports:
  #     - "6379:6379"
  #   networks:
  #     - internal

  # postgres:
  #   image: postgres:18.0
  #   environment:
  #     POSTGRES_USER: pguser
  #     POSTGRES_PASSWORD: pgpass
  #     POSTGRES_DB: managerdb
  #   volumes:
  #     - pgdata:/var/lib/postgresql/data
  #   networks:
  #     - internal

  voiceover:
    build:
      context: .
      dockerfile: services/voiceover/Dockerfile
    
    container_name: voiceover
    
    ports:
      - "8007:8000" 
    
    # Переменные окружения
    environment:
      # Указываем сервису, что нужно использовать CPU
      - SERVICE_BASE_URL=http://voiceover:8000
      - USE_CUDA=False #  отключаем CUDA
      - XTTS_TIMEOUT=900 # Увеличим таймаут, так как CPU медленный
      
      # Настройка кэша Hugging Face/TTS
      - HF_HOME=/app/cache/huggingface
      - TTS_HOME=/app/cache/tts
      
    # Монтирование томов
    volumes:
      - ./audio_files:/app/audio_outputs
      - ./hf_cache:/app/cache/huggingface
      - ./tts_cache:/app/cache/tts
      - ./services/voiceover:/app
      - ./common/models.py:/app/common/models.py:ro
      - ./common/publisher.py:/app/common/publisher.py:ro
      - ./services/common/base_service.py:/app/common/base_service.py:ro
    networks:
      - internal
      - voice

  gigachat-service:
    build:
      context: ./
      dockerfile: services/gigachat/Dockerfile
    environment:
      - PORT=8000
      - GIGACHAT_TOKEN=${GIGACHAT_TOKEN}
      - GIGACHAT_MODEL=GigaChat
      - GIGACHAT_VERIFY_SSL=False
    ports:
      - "8006:8000"   # Different port to avoid conflict
    command: sh -c "sleep 7 && exec python main.py"
    networks:
      - internal

  qwen:
    build:
      context: ./
      dockerfile: services/qwen/Dockerfile
    environment:
      - PORT=8000
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - QWEN_MODEL=qwen/qwen3-30b-a3b:free
      - QWEN_API_BASE=https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation
    ports:
      - "8001:8000"   # Different port to avoid conflict
    command: sh -c "sleep 7 && exec python main.py"
    networks:
      - internal

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G
    networks:
      - ollama
      
  local-llm:
    build:
      context: ./
      dockerfile: services/local-llm/Dockerfile
    ports:
      - "8004:8000"
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - LOCAL_MODEL=llama2:7b  # Модель по умолчанию
    command: sh -c "sleep 7 && exec python main.py"
    depends_on:
      - ollama
    networks:
      - internal
      - ollama

  whisper:
    build:
      context: ./
      dockerfile: services/whisper/Dockerfile
    environment:
      - PORT=8000
      - WHISPER_MODEL=base
      - WHISPER_TIMEOUT=300
    ports:
      - "8005:8000"   # Новый порт для whisper service
    networks:
      - internal
    volumes:
      - ./services/whisper:/app
      #- ./services/whisper:/app/common
      #- ./services/whisper:/app:ro
      - ./common/models.py:/app/common/models.py:ro
      - ./common/publisher.py:/app/common/publisher.py:ro
      - ./services/common/base_service.py:/app/common/base_service.py:ro
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G

  llm-service:
    build:
      context: ./
      dockerfile: services/testing/Dockerfile
    environment:
      - PORT=8000
    ports:
      - "8000:8000"   # только для локальной разработки; в prod можно убрать
    # depends_on:
    #   - redis
    # Это проверка "здоровья" контейнера
    # healthcheck:
    #   test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
    #   interval: 5s
    #   timeout: 3s
    #   retries: 5
    command: sh -c "sleep 7 && exec python main.py"
    networks:
      - internal

  # manager:
  #   build:
  #     context: ./manager
  #     dockerfile: Dockerfile
  #   environment:
  #     - RABBIT_URL=amqp://guest:guest@rabbitmq:5672/
  #     - LLM_URL=http://llm-service:8000/api/v1/infer
  #   depends_on:
  #     - rabbitmq
  #     - llm-service
  #     - postgres
  #     - redis
  #   networks:
  #     - frontend
  #     - internal

  worker:
    build:
      context: .
      dockerfile: worker/Dockerfile
    environment:
      - RABBIT_URL=amqp://guest:guest@rabbitmq:5672/
      - LLM_URL=http://llm-service:8000/api/v1/infer
    depends_on:
      - rabbitmq
      #- llm-service
    command: sh -c "sleep 7 && exec python main.py"

    networks:
      - internal

  wrapper:
    build:
      context: .
      dockerfile: wrapper/Dockerfile
    environment:
      - RABBIT_URL=amqp://guest:guest@rabbitmq:5672/
      - PORT=8003
    ports:
      - "8003:8003"   # только для локальной разработки; в prod можно убрать      
    depends_on:
      - rabbitmq
      #- llm-service
    command: sh -c "sleep 7 && exec python main.py"

    networks:
      - internal
      - frontend


  bot:
    build:
      context: .
      dockerfile: telegram-bot/Dockerfile
    container_name: telegram-bot
    depends_on:
      - rabbitmq
      - wrapper
    environment:
      TELEGRAM_TOKEN: ${TELEGRAM_TOKEN}
      WRAPPER_URL: "http://wrapper:8003"
      BOT_CALLBACK_HOST: "0.0.0.0"
      BOT_CALLBACK_PORT: "9000"
      BOT_CALLBACK_HOST_DOCKER: "telegram-bot"
      # Этот URL вкладывается в задачи как client_callback_url (wrapper будет POSTить сюда)
      CLIENT_CALLBACK_URL_FOR_WRAPPER: "http://telegram-bot:9000/client/webhook"
      POLL_INTERVAL: "1.0"
      GLOBAL_TIMEOUT: "60"
    ports:
      - "9000:9000"  # если вы хотите пробросить для отладки
    networks:
      - frontend
      - voice
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/health"]
      interval: 10s
      timeout: 5s
      retries: 5

networks:
  frontend:
  internal:
  ollama:
  voice:

volumes:
  pgdata:
  ollama_data: {}
